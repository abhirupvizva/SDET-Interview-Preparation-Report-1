<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Senior SDET – 90 Minute Technical Interview Study Guide</title>
    <style>
        :root {
            --primary: #1a365d;
            --secondary: #2c5282;
            --accent: #ed8936;
            --success: #38a169;
            --bg-light: #f7fafc;
            --border: #e2e8f0;
            --text: #2d3748;
            --text-light: #718096;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            color: var(--text);
            background: #fff;
            max-width: 1100px;
            margin: 0 auto;
            padding: 2rem;
        }

        header {
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            padding: 2.5rem;
            border-radius: 12px;
            margin-bottom: 2rem;
            text-align: center;
        }

        header h1 {
            font-size: 2rem;
            margin-bottom: 0.5rem;
        }

        header p {
            opacity: 0.9;
            font-size: 1.1rem;
        }

        .candidate-info {
            background: var(--bg-light);
            border-left: 4px solid var(--accent);
            padding: 1.5rem;
            margin-bottom: 2rem;
            border-radius: 0 8px 8px 0;
        }

        .candidate-info h2 {
            color: var(--primary);
            margin-bottom: 1rem;
        }

        .candidate-info ul {
            list-style: none;
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 0.5rem;
        }

        .candidate-info li::before {
            content: "✓ ";
            color: var(--success);
            font-weight: bold;
        }

        section {
            margin-bottom: 2.5rem;
            border: 1px solid var(--border);
            border-radius: 12px;
            overflow: hidden;
        }

        section > h2 {
            background: var(--primary);
            color: white;
            padding: 1rem 1.5rem;
            font-size: 1.3rem;
            display: flex;
            align-items: center;
            gap: 0.75rem;
        }

        section > h2 .emoji {
            font-size: 1.5rem;
        }

        .section-content {
            padding: 1.5rem;
        }

        h3 {
            color: var(--secondary);
            margin: 1.5rem 0 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 2px solid var(--border);
        }

        h3:first-child {
            margin-top: 0;
        }

        h4 {
            color: var(--primary);
            margin: 1rem 0 0.5rem;
        }

        ul, ol {
            margin-left: 1.5rem;
            margin-bottom: 1rem;
        }

        li {
            margin-bottom: 0.4rem;
        }

        details {
            background: var(--bg-light);
            border: 1px solid var(--border);
            border-radius: 8px;
            margin-bottom: 1rem;
            overflow: hidden;
        }

        summary {
            padding: 1rem 1.25rem;
            cursor: pointer;
            font-weight: 600;
            color: var(--primary);
            background: white;
            border-bottom: 1px solid transparent;
            transition: all 0.2s;
            list-style: none;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        summary::before {
            content: "▶";
            font-size: 0.75rem;
            transition: transform 0.2s;
        }

        details[open] summary::before {
            transform: rotate(90deg);
        }

        summary:hover {
            background: var(--bg-light);
        }

        details[open] summary {
            border-bottom-color: var(--border);
        }

        .answer {
            padding: 1.25rem;
            background: white;
        }

        .answer p {
            margin-bottom: 0.75rem;
        }

        code {
            background: #edf2f7;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.9rem;
            color: #c53030;
        }

        pre {
            background: #1a202c;
            color: #e2e8f0;
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0;
            font-size: 0.85rem;
            line-height: 1.5;
        }

        pre code {
            background: transparent;
            color: inherit;
            padding: 0;
        }

        .highlight {
            background: #fef3c7;
            padding: 0.1rem 0.3rem;
            border-radius: 3px;
        }

        .best-practice {
            background: #c6f6d5;
            border-left: 4px solid var(--success);
            padding: 0.75rem 1rem;
            margin: 0.75rem 0;
            border-radius: 0 6px 6px 0;
        }

        .best-practice::before {
            content: "✅ Best Practice: ";
            font-weight: bold;
            color: var(--success);
        }

        .warning {
            background: #fed7d7;
            border-left: 4px solid #c53030;
            padding: 0.75rem 1rem;
            margin: 0.75rem 0;
            border-radius: 0 6px 6px 0;
        }

        .warning::before {
            content: "⚠️ Common Mistake: ";
            font-weight: bold;
            color: #c53030;
        }

        .time-block {
            display: inline-block;
            background: var(--accent);
            color: white;
            padding: 0.3rem 0.75rem;
            border-radius: 20px;
            font-weight: bold;
            font-size: 0.85rem;
            margin-right: 0.5rem;
        }

        .checklist {
            list-style: none;
            margin-left: 0;
        }

        .checklist li {
            padding: 0.5rem 0;
            padding-left: 2rem;
            position: relative;
            border-bottom: 1px solid var(--border);
        }

        .checklist li::before {
            content: "☐";
            position: absolute;
            left: 0;
            font-size: 1.2rem;
        }

        .star-section {
            margin: 0.75rem 0;
            padding: 0.5rem 0 0.5rem 1rem;
            border-left: 3px solid var(--secondary);
        }

        .star-label {
            font-weight: bold;
            color: var(--secondary);
            display: block;
            font-size: 0.85rem;
            text-transform: uppercase;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }

        th, td {
            border: 1px solid var(--border);
            padding: 0.75rem;
            text-align: left;
        }

        th {
            background: var(--primary);
            color: white;
        }

        tr:nth-child(even) {
            background: var(--bg-light);
        }

        .category-tag {
            display: inline-block;
            background: var(--secondary);
            color: white;
            padding: 0.2rem 0.6rem;
            border-radius: 4px;
            font-size: 0.75rem;
            margin-bottom: 0.5rem;
        }

        .rapid-fire {
            display: grid;
            gap: 0.75rem;
        }

        .rapid-item {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 1rem;
            padding: 1rem;
            background: var(--bg-light);
            border-radius: 8px;
            border-left: 4px solid var(--accent);
        }

        .rapid-q {
            font-weight: 600;
        }

        .rapid-a {
            color: var(--secondary);
        }

        @media print {
            body {
                padding: 0.5rem;
                font-size: 11pt;
            }

            header {
                background: var(--primary) !important;
                -webkit-print-color-adjust: exact;
                print-color-adjust: exact;
            }

            section {
                break-inside: avoid;
            }

            details {
                break-inside: avoid;
            }

            details[open] summary {
                page-break-after: avoid;
            }

            .answer {
                page-break-inside: avoid;
            }

            pre {
                white-space: pre-wrap;
                word-wrap: break-word;
            }
        }

        @media (max-width: 768px) {
            body {
                padding: 1rem;
            }

            header {
                padding: 1.5rem;
            }

            .rapid-item {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Senior SDET – 90 Minute Technical Interview Study Guide</h1>
        <p>VERSANT / Fandango – Streaming & OTT Platform Engineering</p>
    </header>

    <div class="candidate-info">
        <h2>Candidate Profile: Mike (Mustafa Saleh)</h2>
        <ul>
            <li>5+ Years Experience</li>
            <li>Java, Selenium WebDriver</li>
            <li>TestNG, JUnit, Cucumber</li>
            <li>REST Assured, Playwright</li>
            <li>Jenkins, GitLab CI</li>
            <li>PostgreSQL, SQL Server, MySQL</li>
            <li>Kafka Integration Testing</li>
            <li>Healthcare & Consulting Background</li>
        </ul>
    </div>

    <!-- SECTION 1: INTERVIEW BREAKDOWN -->
    <section>
        <h2><span class="emoji">1️⃣</span> Interview Breakdown Overview</h2>
        <div class="section-content">
            <p>A 90-minute Senior SDET interview at a streaming company typically follows this structure:</p>

            <table>
                <thead>
                    <tr>
                        <th>Phase</th>
                        <th>Duration</th>
                        <th>Focus Areas</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Introduction & Background</strong></td>
                        <td><span class="time-block">5-10 min</span></td>
                        <td>Walk through your experience, current role, and why streaming/OTT</td>
                    </tr>
                    <tr>
                        <td><strong>Core Technical Deep-Dive</strong></td>
                        <td><span class="time-block">25-30 min</span></td>
                        <td>Java, automation architecture, Selenium/Playwright, API testing</td>
                    </tr>
                    <tr>
                        <td><strong>Domain-Specific Questions</strong></td>
                        <td><span class="time-block">15-20 min</span></td>
                        <td>Streaming protocols, video playback, cross-device testing, OTT concepts</td>
                    </tr>
                    <tr>
                        <td><strong>Live Coding / Design Exercise</strong></td>
                        <td><span class="time-block">20-25 min</span></td>
                        <td>Framework design, test case creation, debugging scenarios</td>
                    </tr>
                    <tr>
                        <td><strong>Behavioral & Leadership</strong></td>
                        <td><span class="time-block">10-15 min</span></td>
                        <td>Team collaboration, mentorship, quality culture, conflict resolution</td>
                    </tr>
                    <tr>
                        <td><strong>Candidate Questions</strong></td>
                        <td><span class="time-block">5-10 min</span></td>
                        <td>Your questions about team, tech stack, challenges, growth</td>
                    </tr>
                </tbody>
            </table>

            <h3>Key Interviewer Expectations for Senior SDET</h3>
            <ul>
                <li><strong>Technical Depth:</strong> Deep understanding of tools, not just usage</li>
                <li><strong>Architectural Thinking:</strong> Can design scalable test frameworks from scratch</li>
                <li><strong>Problem-Solving:</strong> Debug complex issues across stack layers</li>
                <li><strong>Domain Knowledge:</strong> Understanding of streaming concepts is a differentiator</li>
                <li><strong>Leadership:</strong> Evidence of mentoring, process improvement, and quality advocacy</li>
                <li><strong>Communication:</strong> Clear, structured responses with real examples</li>
            </ul>
        </div>
    </section>

    <!-- SECTION 2: CORE TECHNICAL TOPICS -->
    <section>
        <h2><span class="emoji">2️⃣</span> Core Technical Topics to Study</h2>
        <div class="section-content">
            <p>Use this checklist to ensure comprehensive preparation:</p>

            <h3>Java & Programming Fundamentals</h3>
            <ul class="checklist">
                <li>OOP principles (Encapsulation, Inheritance, Polymorphism, Abstraction)</li>
                <li>SOLID principles and their application in test code</li>
                <li>Collections Framework (List, Set, Map, Queue – when to use each)</li>
                <li>Streams API and functional programming (filter, map, reduce, collect)</li>
                <li>Concurrency basics (threads, synchronization, ExecutorService)</li>
                <li>Exception handling (checked vs unchecked, custom exceptions)</li>
                <li>JVM fundamentals (heap, stack, garbage collection basics)</li>
                <li>Design patterns relevant to testing (Page Object, Factory, Singleton, Builder)</li>
            </ul>

            <h3>Test Automation Architecture</h3>
            <ul class="checklist">
                <li>Page Object Model (POM) and Page Factory patterns</li>
                <li>Test data management strategies (factories, builders, external sources)</li>
                <li>Configuration management (environment-specific configs)</li>
                <li>Reporting frameworks (Allure, ExtentReports, custom reporting)</li>
                <li>Parallel execution strategies and thread safety</li>
                <li>Cross-browser and cross-device architecture</li>
                <li>Framework layers (core, utilities, page objects, tests)</li>
                <li>Dependency injection in test frameworks</li>
            </ul>

            <h3>Selenium WebDriver Internals</h3>
            <ul class="checklist">
                <li>WebDriver architecture and W3C WebDriver protocol</li>
                <li>Explicit, Implicit, and Fluent waits (when to use each)</li>
                <li>Handling dynamic elements, iframes, shadow DOM</li>
                <li>JavaScript execution for complex interactions</li>
                <li>Browser capabilities and options configuration</li>
                <li>Grid architecture for distributed execution</li>
                <li>Handling authentication, cookies, and sessions</li>
                <li>Screenshot capture, video recording strategies</li>
            </ul>

            <h3>Playwright vs Selenium</h3>
            <ul class="checklist">
                <li>Architecture differences (CDP vs WebDriver)</li>
                <li>Auto-waiting and reliability improvements</li>
                <li>Network interception capabilities</li>
                <li>Multi-browser context handling</li>
                <li>Trace viewer and debugging features</li>
                <li>When to choose Playwright over Selenium</li>
            </ul>

            <h3>REST API Testing</h3>
            <ul class="checklist">
                <li>HTTP methods, status codes, headers</li>
                <li>REST Assured fluent syntax and best practices</li>
                <li>Request/response validation (JSON/XML parsing)</li>
                <li>Authentication mechanisms (OAuth2, JWT, API keys)</li>
                <li>API contract testing and schema validation</li>
                <li>Mocking and stubbing with WireMock</li>
                <li>Performance considerations in API tests</li>
            </ul>

            <h3>CI/CD Pipelines</h3>
            <ul class="checklist">
                <li>Jenkins pipeline as code (Jenkinsfile)</li>
                <li>GitLab CI YAML configuration</li>
                <li>Test stage optimization and parallel execution</li>
                <li>Environment provisioning and teardown</li>
                <li>Artifact management and test reporting integration</li>
                <li>Failure handling and retry strategies</li>
                <li>Integration with Docker for test environments</li>
            </ul>

            <h3>Database Testing & SQL</h3>
            <ul class="checklist">
                <li>Complex queries (JOINs, subqueries, aggregations)</li>
                <li>Data validation strategies</li>
                <li>Database state management in tests</li>
                <li>Performance query analysis</li>
                <li>Transaction handling in test context</li>
            </ul>

            <h3>Kafka & Integration Testing</h3>
            <ul class="checklist">
                <li>Kafka architecture (producers, consumers, topics, partitions)</li>
                <li>Testing message production and consumption</li>
                <li>Event ordering and idempotency validation</li>
                <li>Schema registry and Avro/JSON schema testing</li>
                <li>Embedded Kafka for integration tests</li>
            </ul>

            <h3>BDD with Cucumber</h3>
            <ul class="checklist">
                <li>Gherkin syntax best practices</li>
                <li>Step definition organization</li>
                <li>Data tables and scenario outlines</li>
                <li>Hooks and test lifecycle management</li>
                <li>Living documentation generation</li>
            </ul>

            <h3>Streaming & Video Playback Fundamentals</h3>
            <ul class="checklist">
                <li>HLS (HTTP Live Streaming) protocol and manifest files (.m3u8)</li>
                <li>DASH (Dynamic Adaptive Streaming over HTTP)</li>
                <li>Adaptive Bitrate (ABR) streaming and quality switching</li>
                <li>DRM systems (Widevine, FairPlay, PlayReady)</li>
                <li>Video player states and events</li>
                <li>Buffering, seeking, and playback metrics</li>
                <li>CDN concepts and edge caching</li>
                <li>Video codecs (H.264, H.265/HEVC, VP9, AV1)</li>
            </ul>

            <h3>Cross-Device & OTT Testing</h3>
            <ul class="checklist">
                <li>Device-specific challenges (Smart TVs, Roku, Fire TV, Apple TV)</li>
                <li>Remote-based navigation testing</li>
                <li>Device farms and cloud testing platforms</li>
                <li>Performance testing on constrained devices</li>
                <li>Accessibility testing for 10-foot UI</li>
            </ul>

            <h3>Code Quality & SonarQube</h3>
            <ul class="checklist">
                <li>Code coverage metrics interpretation</li>
                <li>Static analysis for test code</li>
                <li>Quality gates configuration</li>
                <li>Technical debt management</li>
            </ul>
        </div>
    </section>

    <!-- SECTION 3: TECHNICAL QUESTIONS -->
    <section>
        <h2><span class="emoji">3️⃣</span> Expected Technical Questions with Detailed Answers</h2>
        <div class="section-content">

            <h3>Java & Programming (Questions 1-8)</h3>

            <details>
                <summary>Q1: Explain the four pillars of OOP and how you apply them in your test framework.</summary>
                <div class="answer">
                    <p><strong>The Four Pillars:</strong></p>
                    <ol>
                        <li><strong>Encapsulation:</strong> Bundling data and methods that operate on that data within a single unit (class), and restricting direct access to some components.</li>
                        <li><strong>Inheritance:</strong> Mechanism where a new class inherits properties and behaviors from an existing class.</li>
                        <li><strong>Polymorphism:</strong> Ability of objects to take on multiple forms – method overloading (compile-time) and overriding (runtime).</li>
                        <li><strong>Abstraction:</strong> Hiding complex implementation details and showing only necessary features.</li>
                    </ol>

                    <p><strong>Application in Test Framework:</strong></p>
<pre><code>// ENCAPSULATION: Page Object with private locators
public class VideoPlayerPage {
    private WebDriver driver;
    private By playButton = By.id("play-btn");

    public void clickPlay() {
        driver.findElement(playButton).click();
    }
}

// INHERITANCE: Base test class
public abstract class BaseTest {
    protected WebDriver driver;

    @BeforeMethod
    public void setup() { /* common setup */ }
}

public class PlaybackTest extends BaseTest {
    // Inherits driver and setup
}

// POLYMORPHISM: Different player implementations
public interface VideoPlayer {
    void play();
    void pause();
}

public class WebPlayer implements VideoPlayer { }
public class MobilePlayer implements VideoPlayer { }

// ABSTRACTION: Abstract base page
public abstract class BasePage {
    protected abstract void waitForPageLoad();
}</code></pre>

                    <div class="best-practice">Keep locators private (encapsulation) and expose only meaningful business methods. Use inheritance judiciously – prefer composition when behavior sharing becomes complex.</div>

                    <div class="warning">Creating deep inheritance hierarchies (more than 2-3 levels) makes code hard to maintain and understand.</div>
                </div>
            </details>

            <details>
                <summary>Q2: When would you use HashMap vs LinkedHashMap vs TreeMap?</summary>
                <div class="answer">
                    <table>
                        <thead>
                            <tr>
                                <th>Map Type</th>
                                <th>Ordering</th>
                                <th>Performance</th>
                                <th>Use Case in Testing</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>HashMap</strong></td>
                                <td>No ordering guaranteed</td>
                                <td>O(1) get/put</td>
                                <td>Caching test data, quick lookups</td>
                            </tr>
                            <tr>
                                <td><strong>LinkedHashMap</strong></td>
                                <td>Insertion order preserved</td>
                                <td>O(1) get/put</td>
                                <td>Maintaining test step order, ordered API payloads</td>
                            </tr>
                            <tr>
                                <td><strong>TreeMap</strong></td>
                                <td>Sorted by key (natural/custom)</td>
                                <td>O(log n) get/put</td>
                                <td>Sorted reports, version comparisons</td>
                            </tr>
                        </tbody>
                    </table>

                    <p><strong>Streaming Platform Example:</strong></p>
<pre><code>// HashMap: Caching video metadata for quick lookup
Map&lt;String, VideoMetadata&gt; videoCache = new HashMap&lt;&gt;();

// LinkedHashMap: Maintaining playback event order for verification
Map&lt;Long, PlaybackEvent&gt; eventTimeline = new LinkedHashMap&lt;&gt;();

// TreeMap: Sorting quality levels for ABR testing
TreeMap&lt;Integer, String&gt; bitrateQualities = new TreeMap&lt;&gt;();
bitrateQualities.put(480, "SD");
bitrateQualities.put(720, "HD");
bitrateQualities.put(1080, "FHD");
// Automatically sorted: 480 → 720 → 1080</code></pre>

                    <div class="best-practice">Use LinkedHashMap when JSON key order matters for API testing or when reproducing exact sequences of events.</div>
                </div>
            </details>

            <details>
                <summary>Q3: Explain Java Streams API. How do you use it for test data processing?</summary>
                <div class="answer">
                    <p>Java Streams API (Java 8+) provides functional-style operations for processing collections. It supports lazy evaluation, parallel processing, and method chaining.</p>

                    <p><strong>Key Operations:</strong></p>
                    <ul>
                        <li><strong>Intermediate:</strong> filter, map, flatMap, sorted, distinct, limit, skip</li>
                        <li><strong>Terminal:</strong> collect, forEach, reduce, count, findFirst, anyMatch, allMatch</li>
                    </ul>

<pre><code>// Real streaming platform test data examples

// Filter videos by genre and get titles
List&lt;String&gt; actionMovies = videos.stream()
    .filter(v -> v.getGenre().equals("Action"))
    .filter(v -> v.getDuration() > 90)
    .map(Video::getTitle)
    .collect(Collectors.toList());

// Group test results by status
Map&lt;TestStatus, List&lt;TestResult&gt;&gt; resultsByStatus = results.stream()
    .collect(Collectors.groupingBy(TestResult::getStatus));

// Validate all playback events have valid timestamps
boolean allValid = playbackEvents.stream()
    .allMatch(event -> event.getTimestamp() > 0);

// Calculate average bitrate from samples
double avgBitrate = bitrateSamples.stream()
    .mapToInt(Sample::getBitrate)
    .average()
    .orElse(0.0);

// Parallel processing for large test data sets
List&lt;ValidationResult&gt; validations = testCases.parallelStream()
    .map(this::validateTestCase)
    .collect(Collectors.toList());</code></pre>

                    <div class="warning">Avoid side effects in stream operations (modifying external state). Streams should be pure transformations.</div>

                    <div class="best-practice">Use parallelStream() only for CPU-intensive operations on large datasets. For small collections or I/O-bound operations, parallel overhead outweighs benefits.</div>
                </div>
            </details>

            <details>
                <summary>Q4: How do you handle concurrent test execution? Explain thread safety concerns.</summary>
                <div class="answer">
                    <p><strong>Key Concerns in Parallel Test Execution:</strong></p>
                    <ol>
                        <li><strong>WebDriver instance isolation:</strong> Each thread must have its own driver</li>
                        <li><strong>Shared test data:</strong> Avoid mutable shared state</li>
                        <li><strong>Resource contention:</strong> Database connections, files, external services</li>
                        <li><strong>Reporting:</strong> Thread-safe report generation</li>
                    </ol>

<pre><code>// Thread-safe WebDriver management using ThreadLocal
public class DriverManager {
    private static ThreadLocal&lt;WebDriver&gt; driverThread = new ThreadLocal&lt;&gt;();

    public static WebDriver getDriver() {
        return driverThread.get();
    }

    public static void setDriver(WebDriver driver) {
        driverThread.set(driver);
    }

    public static void quitDriver() {
        if (driverThread.get() != null) {
            driverThread.get().quit();
            driverThread.remove(); // Prevent memory leaks
        }
    }
}

// TestNG parallel configuration
// testng.xml
&lt;suite name="Streaming Tests" parallel="methods" thread-count="5"&gt;
    &lt;test name="PlaybackTests"&gt;
        &lt;classes&gt;
            &lt;class name="com.streaming.tests.PlaybackTest"/&gt;
        &lt;/classes&gt;
    &lt;/test&gt;
&lt;/suite&gt;

// Thread-safe test data with unique identifiers
public class TestDataFactory {
    public static User createUniqueUser() {
        String uniqueId = UUID.randomUUID().toString().substring(0, 8);
        return new User("testuser_" + uniqueId + "@test.com");
    }
}</code></pre>

                    <div class="best-practice">Always use ThreadLocal for WebDriver in parallel execution. Generate unique test data per test to avoid collisions. Use @BeforeMethod instead of @BeforeClass for test isolation.</div>

                    <div class="warning">Forgetting ThreadLocal.remove() causes memory leaks. Sharing mutable page objects across threads causes race conditions.</div>
                </div>
            </details>

            <details>
                <summary>Q5: What is the difference between checked and unchecked exceptions? How do you handle them in tests?</summary>
                <div class="answer">
                    <table>
                        <thead>
                            <tr>
                                <th>Checked Exceptions</th>
                                <th>Unchecked Exceptions</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Extend Exception class</td>
                                <td>Extend RuntimeException class</td>
                            </tr>
                            <tr>
                                <td>Must be declared or caught at compile time</td>
                                <td>Not checked at compile time</td>
                            </tr>
                            <tr>
                                <td>Examples: IOException, SQLException</td>
                                <td>Examples: NullPointerException, NoSuchElementException</td>
                            </tr>
                        </tbody>
                    </table>

                    <p><strong>Handling in Test Framework:</strong></p>
<pre><code>// Custom test exceptions (unchecked for cleaner test code)
public class PlaybackException extends RuntimeException {
    public PlaybackException(String message, Throwable cause) {
        super(message, cause);
    }
}

public class VideoPlayerPage {
    public void waitForPlayback() {
        try {
            new WebDriverWait(driver, Duration.ofSeconds(30))
                .until(d -> isVideoPlaying());
        } catch (TimeoutException e) {
            throw new PlaybackException(
                "Video failed to start within 30 seconds", e);
        }
    }
}

// Handling expected exceptions in tests
@Test(expectedExceptions = InvalidCredentialsException.class)
public void testLoginWithInvalidCredentials() {
    loginPage.login("invalid@test.com", "wrongpass");
}

// Using assertThrows (JUnit 5)
@Test
public void testPlaybackFailsForBlockedRegion() {
    assertThrows(GeoBlockedException.class, () -> {
        videoPlayer.playContent("blocked-content-id");
    });
}</code></pre>

                    <div class="best-practice">Create custom unchecked exceptions for test frameworks to avoid cluttering test methods with try-catch blocks. Include meaningful messages and original cause for debugging.</div>
                </div>
            </details>

            <details>
                <summary>Q6: Explain the Builder pattern and how you use it for test data creation.</summary>
                <div class="answer">
                    <p>The Builder pattern separates object construction from its representation, allowing the same construction process to create different representations. Essential for creating complex test data with many optional fields.</p>

<pre><code>// Video content builder for test data
public class VideoContent {
    private final String id;
    private final String title;
    private final int duration;
    private final String genre;
    private final boolean hasSubtitles;
    private final List&lt;String&gt; audioTracks;
    private final DrmType drmType;

    private VideoContent(Builder builder) {
        this.id = builder.id;
        this.title = builder.title;
        this.duration = builder.duration;
        this.genre = builder.genre;
        this.hasSubtitles = builder.hasSubtitles;
        this.audioTracks = builder.audioTracks;
        this.drmType = builder.drmType;
    }

    public static class Builder {
        // Required fields
        private final String id;
        private final String title;

        // Optional fields with defaults
        private int duration = 120;
        private String genre = "Drama";
        private boolean hasSubtitles = false;
        private List&lt;String&gt; audioTracks = Arrays.asList("English");
        private DrmType drmType = DrmType.NONE;

        public Builder(String id, String title) {
            this.id = id;
            this.title = title;
        }

        public Builder duration(int minutes) {
            this.duration = minutes;
            return this;
        }

        public Builder genre(String genre) {
            this.genre = genre;
            return this;
        }

        public Builder withSubtitles() {
            this.hasSubtitles = true;
            return this;
        }

        public Builder withDrm(DrmType type) {
            this.drmType = type;
            return this;
        }

        public VideoContent build() {
            return new VideoContent(this);
        }
    }
}

// Usage in tests - clear and readable
VideoContent movie = new VideoContent.Builder("mov123", "Test Movie")
    .duration(150)
    .genre("Action")
    .withSubtitles()
    .withDrm(DrmType.WIDEVINE)
    .build();

VideoContent shortClip = new VideoContent.Builder("clip456", "Short Clip")
    .duration(30)
    .build(); // Uses defaults for other fields</code></pre>

                    <div class="best-practice">Use builders for objects with 4+ fields or many optional parameters. Provide sensible defaults for optional fields to minimize test setup code.</div>
                </div>
            </details>

            <details>
                <summary>Q7: What are the SOLID principles? Give examples in test automation context.</summary>
                <div class="answer">
<pre><code>// S - Single Responsibility Principle
// Each class has one reason to change

// BAD: One class doing everything
public class TestHelper {
    public void clickElement() { }
    public void readExcel() { }
    public void sendEmail() { }
    public void generateReport() { }
}

// GOOD: Separate responsibilities
public class ElementActions { }
public class TestDataReader { }
public class NotificationService { }
public class ReportGenerator { }

// O - Open/Closed Principle
// Open for extension, closed for modification

public interface BrowserFactory {
    WebDriver createDriver();
}

public class ChromeFactory implements BrowserFactory {
    public WebDriver createDriver() { return new ChromeDriver(); }
}

public class FirefoxFactory implements BrowserFactory {
    public WebDriver createDriver() { return new FirefoxDriver(); }
}
// Add new browsers without modifying existing code

// L - Liskov Substitution Principle
// Subtypes must be substitutable for base types

public abstract class VideoTest {
    public abstract void playVideo();
    public void verifyPlayback() { /* common verification */ }
}

public class LiveStreamTest extends VideoTest {
    public void playVideo() { /* live stream specific */ }
}

public class VODTest extends VideoTest {
    public void playVideo() { /* VOD specific */ }
}
// Both can be used anywhere VideoTest is expected

// I - Interface Segregation Principle
// Clients shouldn't depend on interfaces they don't use

// BAD
public interface TestActions {
    void click();
    void type();
    void swipe();
    void pinchZoom();
    void voiceCommand();
}

// GOOD
public interface WebActions { void click(); void type(); }
public interface MobileActions { void swipe(); void pinchZoom(); }
public interface TVActions { void voiceCommand(); }

// D - Dependency Inversion Principle
// Depend on abstractions, not concretions

public class PlaybackValidator {
    private final MetricsCollector collector;

    // Inject dependency via constructor
    public PlaybackValidator(MetricsCollector collector) {
        this.collector = collector;
    }
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q8: Explain garbage collection in Java. When might this matter in test automation?</summary>
                <div class="answer">
                    <p><strong>Garbage Collection Overview:</strong></p>
                    <ul>
                        <li>JVM automatically reclaims memory from objects no longer reachable</li>
                        <li>Generational GC: Young Gen (Eden, Survivor) → Old Gen → Metaspace</li>
                        <li>GC types: Serial, Parallel, CMS, G1, ZGC</li>
                    </ul>

                    <p><strong>When GC Matters in Test Automation:</strong></p>
                    <ol>
                        <li><strong>Memory leaks in long-running test suites:</strong> Not closing drivers, holding references to stale pages</li>
                        <li><strong>Performance testing:</strong> GC pauses can affect timing measurements</li>
                        <li><strong>Large test data processing:</strong> Loading huge files into memory</li>
                        <li><strong>Parallel execution:</strong> Multiple drivers increase heap usage</li>
                    </ol>

<pre><code>// Common memory leak patterns in test frameworks

// LEAK: Static collections that grow indefinitely
private static List&lt;Screenshot&gt; allScreenshots = new ArrayList&lt;&gt;();

// FIX: Clear after each test or use bounded collection
@AfterMethod
public void cleanup() {
    testScreenshots.clear();
}

// LEAK: Not quitting WebDriver
@AfterClass
public void teardown() {
    // driver.quit() missing - leaks browser process and memory
}

// FIX: Always quit in finally block or use try-with-resources pattern
@AfterMethod(alwaysRun = true)
public void quitDriver() {
    DriverManager.quitDriver();
}

// JVM tuning for large test suites
// -Xmx4g -Xms2g -XX:+UseG1GC -XX:MaxGCPauseMillis=200</code></pre>

                    <div class="best-practice">Monitor heap usage during long test runs. Use profilers like VisualVM to identify memory leaks. Implement proper cleanup in @After methods.</div>
                </div>
            </details>

            <h3>Automation Framework Design (Questions 9-16)</h3>

            <details>
                <summary>Q9: Explain the Page Object Model. What are its benefits and common anti-patterns?</summary>
                <div class="answer">
                    <p><strong>Page Object Model (POM)</strong> is a design pattern that creates an object repository for UI elements, separating test logic from page-specific code.</p>

                    <p><strong>Benefits:</strong></p>
                    <ul>
                        <li>Maintainability: UI changes require updates in one place</li>
                        <li>Readability: Tests read like user stories</li>
                        <li>Reusability: Page methods used across multiple tests</li>
                        <li>Abstraction: Tests don't need to know locator strategies</li>
                    </ul>

<pre><code>// Well-designed Page Object for streaming platform
public class VideoPlayerPage extends BasePage {
    // Private locators - encapsulated
    private By playPauseButton = By.cssSelector("[data-testid='play-pause']");
    private By progressBar = By.className("progress-bar");
    private By currentTime = By.id("current-time");
    private By qualitySelector = By.cssSelector(".quality-menu");

    public VideoPlayerPage(WebDriver driver) {
        super(driver);
    }

    // Business-oriented methods - not "clickPlayButton"
    public VideoPlayerPage play() {
        if (!isPlaying()) {
            click(playPauseButton);
            waitForPlaybackToStart();
        }
        return this;
    }

    public VideoPlayerPage seekTo(int seconds) {
        // Complex seek implementation hidden
        JavascriptExecutor js = (JavascriptExecutor) driver;
        js.executeScript("document.querySelector('video').currentTime = " + seconds);
        waitForSeekComplete();
        return this;
    }

    public VideoPlayerPage selectQuality(String quality) {
        click(qualitySelector);
        click(By.xpath("//li[text()='" + quality + "']"));
        return this;
    }

    // State verification methods
    public boolean isPlaying() {
        return !driver.findElement(playPauseButton)
            .getAttribute("class").contains("paused");
    }

    public Duration getCurrentPlaybackTime() {
        String timeText = getText(currentTime);
        return parseTime(timeText);
    }
}

// Test using Page Object - clean and readable
@Test
public void testSeekAndPlay() {
    videoPlayer
        .play()
        .seekTo(60)
        .selectQuality("1080p");

    assertTrue(videoPlayer.isPlaying());
    assertEquals(videoPlayer.getCurrentPlaybackTime().getSeconds(), 60);
}</code></pre>

                    <p><strong>Anti-Patterns to Avoid:</strong></p>
                    <div class="warning">
                        <ul>
                            <li><strong>Exposing WebElements:</strong> <code>public WebElement getPlayButton()</code> breaks encapsulation</li>
                            <li><strong>Assertions in Page Objects:</strong> Keep assertions in test layer only</li>
                            <li><strong>Technical method names:</strong> <code>clickButton()</code> instead of <code>submitOrder()</code></li>
                            <li><strong>Returning void:</strong> Return page objects for fluent chaining</li>
                            <li><strong>Huge page objects:</strong> Split into components for complex pages</li>
                        </ul>
                    </div>
                </div>
            </details>

            <details>
                <summary>Q10: How would you structure a test automation framework from scratch? Describe the layers.</summary>
                <div class="answer">
                    <p><strong>Layered Framework Architecture:</strong></p>

<pre><code>streaming-test-framework/
├── src/
│   ├── main/java/
│   │   └── com.streaming.framework/
│   │       ├── core/                    # LAYER 1: Core Infrastructure
│   │       │   ├── driver/
│   │       │   │   ├── DriverFactory.java
│   │       │   │   ├── DriverManager.java
│   │       │   │   └── BrowserType.java
│   │       │   ├── config/
│   │       │   │   ├── ConfigReader.java
│   │       │   │   └── Environment.java
│   │       │   └── exceptions/
│   │       │       └── FrameworkException.java
│   │       │
│   │       ├── utils/                   # LAYER 2: Utilities
│   │       │   ├── WaitUtils.java
│   │       │   ├── ScreenshotUtils.java
│   │       │   ├── ApiClient.java
│   │       │   ├── DatabaseUtils.java
│   │       │   └── TestDataGenerator.java
│   │       │
│   │       ├── pages/                   # LAYER 3: Page Objects
│   │       │   ├── BasePage.java
│   │       │   ├── web/
│   │       │   │   ├── HomePage.java
│   │       │   │   ├── VideoPlayerPage.java
│   │       │   │   └── SearchPage.java
│   │       │   └── components/
│   │       │       ├── NavigationMenu.java
│   │       │       └── PlayerControls.java
│   │       │
│   │       └── reporting/               # LAYER 4: Reporting
│   │           ├── AllureManager.java
│   │           └── CustomListeners.java
│   │
│   └── test/java/
│       └── com.streaming.tests/         # LAYER 5: Tests
│           ├── base/
│           │   └── BaseTest.java
│           ├── playback/
│           │   ├── PlaybackTests.java
│           │   └── AdPlaybackTests.java
│           ├── search/
│           │   └── SearchTests.java
│           └── api/
│               └── ContentApiTests.java
│
├── src/test/resources/
│   ├── config/
│   │   ├── dev.properties
│   │   ├── staging.properties
│   │   └── prod.properties
│   ├── testdata/
│   │   └── users.json
│   └── testng.xml
│
└── pom.xml</code></pre>

                    <p><strong>Layer Responsibilities:</strong></p>
                    <table>
                        <thead>
                            <tr><th>Layer</th><th>Responsibility</th><th>Key Classes</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Core</td><td>Driver management, configuration, base exceptions</td><td>DriverFactory, ConfigReader</td></tr>
                            <tr><td>Utilities</td><td>Reusable helper methods, cross-cutting concerns</td><td>WaitUtils, ApiClient</td></tr>
                            <tr><td>Page Objects</td><td>UI element interaction, page-specific logic</td><td>VideoPlayerPage, BasePage</td></tr>
                            <tr><td>Reporting</td><td>Test results, screenshots, logging</td><td>AllureManager, Listeners</td></tr>
                            <tr><td>Tests</td><td>Test scenarios, assertions, test data setup</td><td>PlaybackTests, BaseTest</td></tr>
                        </tbody>
                    </table>
                </div>
            </details>

            <details>
                <summary>Q11: How do you handle test data management in your framework?</summary>
                <div class="answer">
                    <p><strong>Test Data Management Strategies:</strong></p>

                    <h4>1. External Data Files (Data-Driven)</h4>
<pre><code>// JSON test data
// users.json
{
  "validUser": {
    "email": "premium@test.com",
    "password": "Test123!",
    "subscriptionType": "PREMIUM"
  },
  "expiredUser": {
    "email": "expired@test.com",
    "password": "Test123!",
    "subscriptionType": "EXPIRED"
  }
}

// TestData reader
public class TestDataReader {
    private static ObjectMapper mapper = new ObjectMapper();

    public static &lt;T&gt; T getData(String fileName, Class&lt;T&gt; clazz) {
        return mapper.readValue(
            new File("src/test/resources/testdata/" + fileName),
            clazz
        );
    }
}</code></pre>

                    <h4>2. Builder/Factory Pattern (Dynamic)</h4>
<pre><code>public class UserFactory {
    public static User createPremiumUser() {
        return User.builder()
            .email(generateUniqueEmail())
            .password("Test123!")
            .subscriptionType(SubscriptionType.PREMIUM)
            .build();
    }

    public static User createUserWithExpiredSubscription() {
        User user = createPremiumUser();
        // Use API to expire subscription
        SubscriptionApi.expireSubscription(user.getId());
        return user;
    }
}</code></pre>

                    <h4>3. Database Seeding</h4>
<pre><code>@BeforeClass
public void setupTestData() {
    // Insert test data directly for state-dependent tests
    jdbcTemplate.execute(
        "INSERT INTO users (id, email, subscription_expires) " +
        "VALUES ('test-123', 'test@example.com', CURRENT_DATE - 1)"
    );
}

@AfterClass
public void cleanupTestData() {
    jdbcTemplate.execute("DELETE FROM users WHERE email LIKE '%@test.com'");
}</code></pre>

                    <h4>4. API-Based Setup</h4>
<pre><code>@BeforeMethod
public void createTestContent() {
    // Create video content via API for test
    contentId = ContentApi.createVideo(
        VideoContent.builder()
            .title("Test Video " + System.currentTimeMillis())
            .duration(300)
            .build()
    );
}

@AfterMethod
public void deleteTestContent() {
    ContentApi.deleteVideo(contentId);
}</code></pre>

                    <div class="best-practice">Use API/DB setup for faster test data creation. Generate unique identifiers to enable parallel execution. Clean up test data to prevent test pollution.</div>
                </div>
            </details>

            <details>
                <summary>Q12: Explain explicit, implicit, and fluent waits. When do you use each?</summary>
                <div class="answer">
                    <table>
                        <thead>
                            <tr>
                                <th>Wait Type</th>
                                <th>Scope</th>
                                <th>Use Case</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Implicit Wait</strong></td>
                                <td>Global - applies to all findElement calls</td>
                                <td>Legacy code, simple projects</td>
                            </tr>
                            <tr>
                                <td><strong>Explicit Wait</strong></td>
                                <td>Specific condition - applies to one element/condition</td>
                                <td>Production frameworks - recommended</td>
                            </tr>
                            <tr>
                                <td><strong>Fluent Wait</strong></td>
                                <td>Customizable explicit wait with polling and exception handling</td>
                                <td>Complex conditions, custom polling intervals</td>
                            </tr>
                        </tbody>
                    </table>

<pre><code>// IMPLICIT WAIT - Avoid mixing with explicit waits
driver.manage().timeouts().implicitlyWait(Duration.ofSeconds(10));
// Problem: Applies to ALL findElement calls, can mask issues

// EXPLICIT WAIT - Preferred approach
WebDriverWait wait = new WebDriverWait(driver, Duration.ofSeconds(20));

// Wait for element to be clickable
WebElement playButton = wait.until(
    ExpectedConditions.elementToBeClickable(By.id("play"))
);

// Wait for video to start playing (custom condition)
wait.until(driver -> {
    String playState = driver.findElement(By.tagName("video"))
        .getAttribute("paused");
    return "false".equals(playState);
});

// FLUENT WAIT - Maximum customization
Wait&lt;WebDriver&gt; fluentWait = new FluentWait&lt;&gt;(driver)
    .withTimeout(Duration.ofSeconds(30))
    .pollingEvery(Duration.ofMillis(500))
    .ignoring(NoSuchElementException.class)
    .ignoring(StaleElementReferenceException.class)
    .withMessage("Video player did not load");

WebElement player = fluentWait.until(driver ->
    driver.findElement(By.id("video-player"))
);

// Streaming-specific: Wait for buffering to complete
public void waitForBufferingComplete() {
    new WebDriverWait(driver, Duration.ofSeconds(60))
        .until(d -> {
            String bufferState = (String) ((JavascriptExecutor) d)
                .executeScript("return document.querySelector('video').readyState");
            return Integer.parseInt(bufferState) >= 3; // HAVE_FUTURE_DATA
        });
}</code></pre>

                    <div class="warning">Never mix implicit and explicit waits - leads to unpredictable timeout behavior and can cause waits to stack (e.g., 10s implicit + 20s explicit = 30s total in some scenarios).</div>

                    <div class="best-practice">Create a WaitUtils class with reusable wait methods specific to your application. For streaming, create waits for video states, buffering, and quality changes.</div>
                </div>
            </details>

            <details>
                <summary>Q13: How do you implement cross-browser testing in your framework?</summary>
                <div class="answer">
<pre><code>// Browser Factory using Factory Pattern
public class DriverFactory {

    public static WebDriver createDriver(BrowserType browser, boolean headless) {
        switch (browser) {
            case CHROME:
                return createChromeDriver(headless);
            case FIREFOX:
                return createFirefoxDriver(headless);
            case SAFARI:
                return createSafariDriver();
            case EDGE:
                return createEdgeDriver(headless);
            default:
                throw new IllegalArgumentException("Unsupported browser: " + browser);
        }
    }

    private static WebDriver createChromeDriver(boolean headless) {
        ChromeOptions options = new ChromeOptions();
        if (headless) {
            options.addArguments("--headless=new");
        }
        // Streaming-specific options
        options.addArguments("--autoplay-policy=no-user-gesture-required");
        options.addArguments("--disable-web-security"); // For DRM testing
        options.setExperimentalOption("excludeSwitches",
            Arrays.asList("enable-automation"));

        return new ChromeDriver(options);
    }

    private static WebDriver createFirefoxDriver(boolean headless) {
        FirefoxOptions options = new FirefoxOptions();
        if (headless) {
            options.addArguments("-headless");
        }
        // Enable DRM for Firefox
        options.addPreference("media.eme.enabled", true);
        options.addPreference("media.gmp-manager.updateEnabled", true);

        return new FirefoxDriver(options);
    }
}

// Configuration-driven browser selection
// config.properties
browser=chrome
headless=false
grid.enabled=true
grid.url=http://selenium-grid:4444

// Base Test reading config
public class BaseTest {
    @BeforeMethod
    @Parameters({"browser"})
    public void setup(@Optional("chrome") String browser) {
        BrowserType browserType = BrowserType.valueOf(browser.toUpperCase());
        WebDriver driver;

        if (Config.isGridEnabled()) {
            driver = createRemoteDriver(browserType, Config.getGridUrl());
        } else {
            driver = DriverFactory.createDriver(browserType, Config.isHeadless());
        }

        DriverManager.setDriver(driver);
    }
}

// TestNG XML for parallel cross-browser
&lt;suite name="Cross Browser Suite" parallel="tests" thread-count="3"&gt;
    &lt;test name="Chrome Tests"&gt;
        &lt;parameter name="browser" value="chrome"/&gt;
        &lt;classes&gt;&lt;class name="PlaybackTests"/&gt;&lt;/classes&gt;
    &lt;/test&gt;
    &lt;test name="Firefox Tests"&gt;
        &lt;parameter name="browser" value="firefox"/&gt;
        &lt;classes&gt;&lt;class name="PlaybackTests"/&gt;&lt;/classes&gt;
    &lt;/test&gt;
    &lt;test name="Safari Tests"&gt;
        &lt;parameter name="browser" value="safari"/&gt;
        &lt;classes&gt;&lt;class name="PlaybackTests"/&gt;&lt;/classes&gt;
    &lt;/test&gt;
&lt;/suite&gt;</code></pre>

                    <div class="best-practice">For streaming platforms, test EME (Encrypted Media Extensions) compatibility across browsers. Safari and Firefox have different DRM implementations than Chrome.</div>
                </div>
            </details>

            <details>
                <summary>Q14: How do you make your test framework configuration-driven?</summary>
                <div class="answer">
<pre><code>// Multi-environment configuration structure
src/test/resources/config/
├── default.properties      # Base configuration
├── dev.properties          # Development overrides
├── staging.properties      # Staging overrides
└── prod.properties         # Production (read-only tests)

// default.properties
app.base.url=https://dev.streaming.com
api.base.url=https://api.dev.streaming.com
implicit.wait.seconds=10
explicit.wait.seconds=30
screenshot.on.failure=true
video.record=false
browser=chrome
headless=false

// staging.properties
app.base.url=https://staging.streaming.com
api.base.url=https://api.staging.streaming.com
video.record=true

// ConfigReader with environment support
public class ConfigReader {
    private static Properties properties = new Properties();
    private static final String ENV = System.getProperty("env", "dev");

    static {
        loadProperties("config/default.properties");
        loadProperties("config/" + ENV + ".properties");
    }

    private static void loadProperties(String path) {
        try (InputStream input = ConfigReader.class.getClassLoader()
                .getResourceAsStream(path)) {
            if (input != null) {
                properties.load(input);
            }
        } catch (IOException e) {
            throw new RuntimeException("Failed to load config: " + path, e);
        }
    }

    public static String get(String key) {
        // Environment variable override > property file
        String envValue = System.getenv(key.replace(".", "_").toUpperCase());
        return envValue != null ? envValue : properties.getProperty(key);
    }

    public static String getBaseUrl() {
        return get("app.base.url");
    }

    public static int getExplicitWait() {
        return Integer.parseInt(get("explicit.wait.seconds"));
    }
}

// Running tests with different environments
// Maven: mvn test -Denv=staging
// Jenkins: Set ENV variable in pipeline

// Jenkinsfile
pipeline {
    parameters {
        choice(name: 'ENVIRONMENT', choices: ['dev', 'staging', 'prod'])
    }
    stages {
        stage('Test') {
            steps {
                sh "mvn test -Denv=${params.ENVIRONMENT}"
            }
        }
    }
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q15: How do you handle reporting in your automation framework?</summary>
                <div class="answer">
<pre><code>// Allure Reporting Integration (Industry standard)

// Maven dependency
&lt;dependency&gt;
    &lt;groupId&gt;io.qameta.allure&lt;/groupId&gt;
    &lt;artifactId&gt;allure-testng&lt;/artifactId&gt;
    &lt;version&gt;2.24.0&lt;/version&gt;
&lt;/dependency&gt;

// Test with Allure annotations
@Epic("Video Playback")
@Feature("Basic Playback Controls")
public class PlaybackTests extends BaseTest {

    @Test
    @Story("User can play video content")
    @Severity(SeverityLevel.CRITICAL)
    @Description("Verify that video playback starts when play button is clicked")
    public void testPlaybackStarts() {
        // Test implementation
    }

    @Step("Navigate to video page: {videoId}")
    public void navigateToVideo(String videoId) {
        Allure.addAttachment("Video ID", videoId);
        // Navigation code
    }
}

// Custom Test Listener for automatic screenshot on failure
public class AllureTestListener implements ITestListener {

    @Override
    public void onTestFailure(ITestResult result) {
        WebDriver driver = DriverManager.getDriver();
        if (driver != null) {
            byte[] screenshot = ((TakesScreenshot) driver)
                .getScreenshotAs(OutputType.BYTES);
            Allure.addAttachment("Failure Screenshot",
                "image/png",
                new ByteArrayInputStream(screenshot),
                "png"
            );

            // For video playback tests, capture player state
            capturePlayerState(driver);
        }
    }

    private void capturePlayerState(WebDriver driver) {
        try {
            String playerState = (String) ((JavascriptExecutor) driver)
                .executeScript(
                    "var v = document.querySelector('video');" +
                    "return JSON.stringify({" +
                    "  currentTime: v.currentTime," +
                    "  duration: v.duration," +
                    "  paused: v.paused," +
                    "  readyState: v.readyState," +
                    "  networkState: v.networkState," +
                    "  error: v.error ? v.error.message : null" +
                    "});"
                );
            Allure.addAttachment("Player State", "application/json", playerState);
        } catch (Exception e) {
            // Player might not be present
        }
    }
}

// Generating report
// mvn allure:serve    - Generate and open in browser
// mvn allure:report   - Generate static report</code></pre>

                    <div class="best-practice">For streaming tests, attach video player state, network conditions, and buffering metrics to failure reports. This dramatically speeds up debugging.</div>
                </div>
            </details>

            <details>
                <summary>Q16: How do you handle flaky tests in your framework?</summary>
                <div class="answer">
                    <p><strong>Root Causes of Flakiness in Streaming Tests:</strong></p>
                    <ul>
                        <li>Network latency affecting video buffering</li>
                        <li>CDN cache inconsistencies</li>
                        <li>Timing issues with player state transitions</li>
                        <li>Dynamic ad insertion timing</li>
                        <li>Device resource contention</li>
                    </ul>

<pre><code>// 1. RETRY MECHANISM (TestNG)
@Test(retryAnalyzer = RetryAnalyzer.class)
public void testVideoPlayback() { }

public class RetryAnalyzer implements IRetryAnalyzer {
    private int retryCount = 0;
    private static final int MAX_RETRY = 2;

    @Override
    public boolean retry(ITestResult result) {
        if (retryCount < MAX_RETRY) {
            retryCount++;
            return true;
        }
        return false;
    }
}

// 2. ROBUST WAITING STRATEGIES
public void waitForVideoToPlay() {
    new WebDriverWait(driver, Duration.ofSeconds(30))
        .pollingEvery(Duration.ofMillis(500))
        .ignoring(StaleElementReferenceException.class)
        .until(driver -> {
            try {
                String state = (String) js.executeScript(
                    "return document.querySelector('video').paused"
                );
                return "false".equals(state);
            } catch (Exception e) {
                return false;
            }
        });
}

// 3. SELF-HEALING LOCATORS
public class SmartLocator {
    private By primaryLocator;
    private List&lt;By&gt; fallbackLocators;

    public WebElement find(WebDriver driver) {
        try {
            return driver.findElement(primaryLocator);
        } catch (NoSuchElementException e) {
            for (By fallback : fallbackLocators) {
                try {
                    return driver.findElement(fallback);
                } catch (NoSuchElementException ignored) {}
            }
            throw new NoSuchElementException("Element not found with any locator");
        }
    }
}

// 4. ISOLATION - Reset state before each test
@BeforeMethod
public void resetPlayerState() {
    // Clear cookies, local storage
    driver.manage().deleteAllCookies();
    js.executeScript("localStorage.clear(); sessionStorage.clear();");

    // Reset any cached video state
    apiClient.post("/test/reset-user-state/" + testUserId);
}

// 5. CONDITIONAL WAITS for streaming-specific states
public void waitForAdComplete() {
    new WebDriverWait(driver, Duration.ofSeconds(120))
        .until(driver -> {
            // Check if still in ad break
            Boolean isAd = (Boolean) js.executeScript(
                "return window.player && window.player.isPlayingAd()"
            );
            return !Boolean.TRUE.equals(isAd);
        });
}</code></pre>

                    <div class="best-practice">Track flaky test metrics. If a test fails 1 in 10 runs, it's costing 10% of your pipeline reliability. Prioritize fixing or quarantining flaky tests.</div>
                </div>
            </details>

            <h3>Selenium & Playwright (Questions 17-22)</h3>

            <details>
                <summary>Q17: Explain Selenium WebDriver architecture and the W3C WebDriver protocol.</summary>
                <div class="answer">
                    <p><strong>Selenium WebDriver Architecture:</strong></p>
                    <ol>
                        <li><strong>Client Libraries:</strong> Java/Python/C# bindings that create JSON commands</li>
                        <li><strong>WebDriver Protocol:</strong> HTTP-based protocol (W3C standard)</li>
                        <li><strong>Browser Drivers:</strong> ChromeDriver, GeckoDriver translate commands</li>
                        <li><strong>Browsers:</strong> Execute commands and return responses</li>
                    </ol>

<pre><code>// Request/Response flow example
Client Code: driver.findElement(By.id("play"))

↓ Serialized to HTTP Request

POST /session/{sessionId}/element
{
  "using": "css selector",
  "value": "#play"
}

↓ ChromeDriver receives and translates to Chrome DevTools Protocol

↓ Browser executes and returns

← Response:
{
  "value": {
    "element-6066-11e4-a52e-4f735466cecf": "abc123"
  }
}

← Client receives element reference</code></pre>

                    <p><strong>W3C WebDriver Protocol Key Endpoints:</strong></p>
                    <table>
                        <thead>
                            <tr><th>Action</th><th>Method</th><th>Endpoint</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Create Session</td><td>POST</td><td>/session</td></tr>
                            <tr><td>Navigate</td><td>POST</td><td>/session/{id}/url</td></tr>
                            <tr><td>Find Element</td><td>POST</td><td>/session/{id}/element</td></tr>
                            <tr><td>Click</td><td>POST</td><td>/session/{id}/element/{elementId}/click</td></tr>
                            <tr><td>Screenshot</td><td>GET</td><td>/session/{id}/screenshot</td></tr>
                        </tbody>
                    </table>

                    <div class="best-practice">Understanding the protocol helps debug connection issues. Use browser devtools Network tab to inspect WebDriver commands when troubleshooting.</div>
                </div>
            </details>

            <details>
                <summary>Q18: How do you handle Shadow DOM elements in Selenium?</summary>
                <div class="answer">
                    <p>Shadow DOM creates encapsulated DOM trees that regular locators can't penetrate. Common in modern web components like video players.</p>

<pre><code>// Selenium 4+ approach - getShadowRoot()
// HTML structure:
// &lt;custom-player&gt;
//   #shadow-root
//     &lt;button id="play"&gt;Play&lt;/button&gt;

WebElement hostElement = driver.findElement(By.tagName("custom-player"));
SearchContext shadowRoot = hostElement.getShadowRoot();
WebElement playButton = shadowRoot.findElement(By.id("play"));

// Nested shadow DOM
WebElement outerHost = driver.findElement(By.id("player-container"));
SearchContext outerShadow = outerHost.getShadowRoot();
WebElement innerHost = outerShadow.findElement(By.cssSelector("video-controls"));
SearchContext innerShadow = innerHost.getShadowRoot();
WebElement muteButton = innerShadow.findElement(By.id("mute"));

// JavaScript approach (works for open shadow roots)
public WebElement findInShadowDom(String hostSelector, String elementSelector) {
    String script =
        "return document.querySelector('" + hostSelector + "')" +
        ".shadowRoot.querySelector('" + elementSelector + "')";
    return (WebElement) js.executeScript(script);
}

// Reusable utility for deep shadow DOM traversal
public WebElement findDeepShadowElement(String... selectors) {
    StringBuilder script = new StringBuilder("return document");
    for (int i = 0; i < selectors.length; i++) {
        if (i < selectors.length - 1) {
            script.append(".querySelector('").append(selectors[i]).append("').shadowRoot");
        } else {
            script.append(".querySelector('").append(selectors[i]).append("')");
        }
    }
    return (WebElement) js.executeScript(script.toString());
}

// Usage:
WebElement btn = findDeepShadowElement("player-container", "video-controls", "#play");</code></pre>

                    <div class="warning">Closed Shadow DOM cannot be accessed programmatically. If you control the app, consider adding test hooks or using open shadow roots in test environments.</div>
                </div>
            </details>

            <details>
                <summary>Q19: When would you choose Playwright over Selenium? Compare their architectures.</summary>
                <div class="answer">
                    <table>
                        <thead>
                            <tr><th>Aspect</th><th>Selenium</th><th>Playwright</th></tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Protocol</strong></td>
                                <td>W3C WebDriver (HTTP)</td>
                                <td>Chrome DevTools Protocol (WebSocket)</td>
                            </tr>
                            <tr>
                                <td><strong>Auto-wait</strong></td>
                                <td>Manual waits required</td>
                                <td>Built-in auto-waiting</td>
                            </tr>
                            <tr>
                                <td><strong>Network Interception</strong></td>
                                <td>Limited (BrowserMob Proxy)</td>
                                <td>Native API support</td>
                            </tr>
                            <tr>
                                <td><strong>Multi-tab/Context</strong></td>
                                <td>Complex handling</td>
                                <td>First-class support</td>
                            </tr>
                            <tr>
                                <td><strong>Mobile</strong></td>
                                <td>Via Appium</td>
                                <td>Device emulation only</td>
                            </tr>
                            <tr>
                                <td><strong>Browser Support</strong></td>
                                <td>All major + legacy</td>
                                <td>Chromium, Firefox, WebKit</td>
                            </tr>
                            <tr>
                                <td><strong>Language Support</strong></td>
                                <td>Java, Python, C#, JS, Ruby</td>
                                <td>JS/TS, Python, Java, C#</td>
                            </tr>
                        </tbody>
                    </table>

<pre><code>// Playwright advantages for streaming platforms

// 1. Network interception - Mock API responses, simulate slow networks
await page.route('**/api/content/**', route => {
    route.fulfill({
        status: 200,
        body: JSON.stringify(mockContentData)
    });
});

// 2. Simulate network conditions
const context = await browser.newContext({
    offline: false
});
await context.setOffline(true); // Simulate network loss during playback

// 3. Multi-context for testing multiple users simultaneously
const subscriberContext = await browser.newContext();
const guestContext = await browser.newContext();
// Test same content with different access levels

// 4. Trace viewer for debugging
await context.tracing.start({ screenshots: true, snapshots: true });
// ... test execution
await context.tracing.stop({ path: 'trace.zip' });
// npx playwright show-trace trace.zip

// 5. Video recording built-in
const context = await browser.newContext({
    recordVideo: { dir: 'videos/' }
});</code></pre>

                    <p><strong>Choose Playwright when:</strong></p>
                    <ul>
                        <li>Network mocking is critical (streaming API testing)</li>
                        <li>Need reliable auto-waiting</li>
                        <li>Testing offline/network scenarios</li>
                        <li>Modern browser-only testing</li>
                    </ul>

                    <p><strong>Choose Selenium when:</strong></p>
                    <ul>
                        <li>Need legacy browser support (IE11)</li>
                        <li>Existing large Selenium codebase</li>
                        <li>Real mobile device testing (with Appium)</li>
                        <li>Need Selenium Grid infrastructure</li>
                    </ul>
                </div>
            </details>

            <details>
                <summary>Q20: How do you handle iframes in Selenium, especially for video players?</summary>
                <div class="answer">
<pre><code>// Video players often render in iframes (especially embedded players)

// Switch to iframe by WebElement
WebElement playerFrame = driver.findElement(By.id("video-player-frame"));
driver.switchTo().frame(playerFrame);

// Now interact with elements inside iframe
driver.findElement(By.id("play-button")).click();

// Switch back to main document
driver.switchTo().defaultContent();

// Nested iframes (common in third-party players)
driver.switchTo().frame("outer-frame");
driver.switchTo().frame("inner-frame");
// interact...
driver.switchTo().parentFrame(); // Go up one level
driver.switchTo().defaultContent(); // Go to top

// Robust iframe handler
public class IframeHandler {
    private WebDriver driver;
    private WebDriverWait wait;

    public void switchToFrame(By locator) {
        wait.until(ExpectedConditions.frameToBeAvailableAndSwitchToIt(locator));
    }

    public &lt;T&gt; T executeInFrame(By frameLocator, Supplier&lt;T&gt; action) {
        try {
            switchToFrame(frameLocator);
            return action.get();
        } finally {
            driver.switchTo().defaultContent();
        }
    }
}

// Usage
String playerState = iframeHandler.executeInFrame(
    By.id("player-iframe"),
    () -> driver.findElement(By.id("status")).getText()
);

// Handling dynamically loaded iframes (common for ads)
public void waitForAdIframeAndSkip() {
    try {
        wait.until(ExpectedConditions.frameToBeAvailableAndSwitchToIt(
            By.cssSelector("iframe[id^='ad-frame']")));

        WebElement skipButton = wait.until(
            ExpectedConditions.elementToBeClickable(By.className("skip-ad")));
        skipButton.click();
    } finally {
        driver.switchTo().defaultContent();
    }
}</code></pre>

                    <div class="warning">Always switch back to default content in a finally block. Forgetting to switch back is a common cause of "element not found" errors in subsequent test steps.</div>
                </div>
            </details>

            <details>
                <summary>Q21: How do you capture and validate video player metrics using Selenium?</summary>
                <div class="answer">
<pre><code>// JavaScript execution to extract HTML5 video metrics

public class VideoPlayerMetrics {
    private JavascriptExecutor js;

    // Get comprehensive video state
    public Map&lt;String, Object&gt; getVideoState() {
        String script =
            "var v = document.querySelector('video');" +
            "return {" +
            "  currentTime: v.currentTime," +
            "  duration: v.duration," +
            "  paused: v.paused," +
            "  ended: v.ended," +
            "  muted: v.muted," +
            "  volume: v.volume," +
            "  playbackRate: v.playbackRate," +
            "  readyState: v.readyState," +
            "  networkState: v.networkState," +
            "  buffered: v.buffered.length > 0 ? v.buffered.end(0) : 0," +
            "  videoWidth: v.videoWidth," +
            "  videoHeight: v.videoHeight," +
            "  error: v.error ? v.error.code : null" +
            "};";

        return (Map&lt;String, Object&gt;) js.executeScript(script);
    }

    // Check for quality (resolution)
    public String getCurrentQuality() {
        return (String) js.executeScript(
            "var v = document.querySelector('video');" +
            "return v.videoHeight + 'p';"
        );
    }

    // Get buffered ranges
    public List&lt;double[]&gt; getBufferedRanges() {
        return (List&lt;double[]&gt;) js.executeScript(
            "var v = document.querySelector('video');" +
            "var ranges = [];" +
            "for (var i = 0; i < v.buffered.length; i++) {" +
            "  ranges.push([v.buffered.start(i), v.buffered.end(i)]);" +
            "}" +
            "return ranges;"
        );
    }

    // Wait for specific quality
    public void waitForQuality(int expectedHeight) {
        new WebDriverWait(driver, Duration.ofSeconds(30))
            .until(d -> {
                Long height = (Long) js.executeScript(
                    "return document.querySelector('video').videoHeight"
                );
                return height != null && height >= expectedHeight;
            });
    }
}

// Integration with custom player APIs (if exposed)
public void validateStreamingMetrics() {
    // Many players expose custom APIs
    Map&lt;String, Object&gt; metrics = (Map&lt;String, Object&gt;) js.executeScript(
        "return window.player.getPlaybackMetrics();"
    );

    assertThat((Double) metrics.get("droppedFrames")).isLessThan(10);
    assertThat((Double) metrics.get("bufferingTime")).isLessThan(5000);
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q22: How do you handle dynamic elements and StaleElementReferenceException?</summary>
                <div class="answer">
                    <p><strong>StaleElementReferenceException</strong> occurs when a previously found element is no longer attached to the DOM (page refreshed, element re-rendered, AJAX update).</p>

<pre><code>// Strategy 1: Re-find element when needed
public void clickWithRetry(By locator, int maxAttempts) {
    for (int attempt = 1; attempt <= maxAttempts; attempt++) {
        try {
            driver.findElement(locator).click();
            return;
        } catch (StaleElementReferenceException e) {
            if (attempt == maxAttempts) throw e;
            // Wait briefly before retry
            sleep(500);
        }
    }
}

// Strategy 2: Use fresh locator each time (don't store WebElement)
// BAD - storing element reference
private WebElement playButton = driver.findElement(By.id("play"));

// GOOD - find element when needed
public By playButton = By.id("play");
public void clickPlay() {
    driver.findElement(playButton).click();
}

// Strategy 3: Wait for element to be stable
public WebElement waitForStableElement(By locator) {
    return new FluentWait&lt;&gt;(driver)
        .withTimeout(Duration.ofSeconds(10))
        .pollingEvery(Duration.ofMillis(500))
        .ignoring(StaleElementReferenceException.class)
        .until(d -> {
            WebElement element = d.findElement(locator);
            // Verify element is interactable
            return element.isDisplayed() && element.isEnabled() ? element : null;
        });
}

// Strategy 4: For dynamically changing elements (like progress bar)
public void waitForProgressUpdate(By locator, String previousValue) {
    new WebDriverWait(driver, Duration.ofSeconds(30))
        .ignoring(StaleElementReferenceException.class)
        .until(d -> {
            String currentValue = d.findElement(locator).getText();
            return !currentValue.equals(previousValue);
        });
}

// Strategy 5: Wrapper method that handles staleness automatically
public void safeClick(By locator) {
    new WebDriverWait(driver, Duration.ofSeconds(10))
        .ignoring(StaleElementReferenceException.class)
        .ignoring(ElementClickInterceptedException.class)
        .until(d -> {
            try {
                d.findElement(locator).click();
                return true;
            } catch (StaleElementReferenceException e) {
                return false;
            }
        });
}</code></pre>

                    <div class="best-practice">In streaming UIs, elements update frequently during playback. Never cache WebElement references for dynamic elements like progress bars, quality indicators, or ad overlays.</div>
                </div>
            </details>

            <h3>API & Backend Testing (Questions 23-28)</h3>

            <details>
                <summary>Q23: How do you structure API tests using REST Assured?</summary>
                <div class="answer">
<pre><code>// Base specification for reusability
public class ApiConfig {
    public static RequestSpecification getBaseSpec() {
        return new RequestSpecBuilder()
            .setBaseUri(Config.getApiBaseUrl())
            .setContentType(ContentType.JSON)
            .addFilter(new AllureRestAssured()) // Reporting
            .addFilter(new RequestLoggingFilter())
            .addFilter(new ResponseLoggingFilter())
            .build();
    }

    public static RequestSpecification getAuthenticatedSpec(String token) {
        return new RequestSpecBuilder()
            .addRequestSpecification(getBaseSpec())
            .addHeader("Authorization", "Bearer " + token)
            .build();
    }
}

// API client for streaming service
public class ContentApi {

    public static ContentResponse getContentById(String contentId) {
        return given()
            .spec(ApiConfig.getAuthenticatedSpec(getAuthToken()))
        .when()
            .get("/v1/content/{id}", contentId)
        .then()
            .statusCode(200)
            .extract()
            .as(ContentResponse.class);
    }

    public static ValidatableResponse searchContent(String query, int page) {
        return given()
            .spec(ApiConfig.getAuthenticatedSpec(getAuthToken()))
            .queryParam("q", query)
            .queryParam("page", page)
            .queryParam("limit", 20)
        .when()
            .get("/v1/search")
        .then()
            .statusCode(200)
            .body("results", not(empty()))
            .body("pagination.page", equalTo(page));
    }

    public static void validateStreamingManifest(String contentId) {
        String manifestUrl = given()
            .spec(ApiConfig.getAuthenticatedSpec(getAuthToken()))
        .when()
            .get("/v1/content/{id}/playback", contentId)
        .then()
            .statusCode(200)
            .body("manifestUrl", notNullValue())
            .extract()
            .path("manifestUrl");

        // Validate manifest is accessible
        given()
            .relaxedHTTPSValidation()
        .when()
            .get(manifestUrl)
        .then()
            .statusCode(200)
            .contentType(containsString("mpegurl")); // HLS manifest
    }
}

// Comprehensive test example
@Test
public void testContentPlaybackFlow() {
    // 1. Search for content
    String contentId = ContentApi.searchContent("test movie", 1)
        .extract()
        .path("results[0].id");

    // 2. Get content details
    ContentResponse content = ContentApi.getContentById(contentId);
    assertThat(content.getDrmType()).isEqualTo("WIDEVINE");

    // 3. Get playback session
    given()
        .spec(ApiConfig.getAuthenticatedSpec(getAuthToken()))
        .body(new PlaybackRequest(contentId, "WEB"))
    .when()
        .post("/v1/playback/session")
    .then()
        .statusCode(200)
        .body("licenseUrl", notNullValue())
        .body("manifestUrl", notNullValue())
        .body("sessionId", matchesPattern("[a-f0-9-]{36}"));
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q24: Explain OAuth2 / JWT authentication and how you test authenticated APIs.</summary>
                <div class="answer">
                    <p><strong>OAuth2 Flows:</strong></p>
                    <ul>
                        <li><strong>Authorization Code:</strong> Web apps (user redirected to auth server)</li>
                        <li><strong>Client Credentials:</strong> Machine-to-machine (no user)</li>
                        <li><strong>Password Grant:</strong> Direct username/password (testing, legacy)</li>
                        <li><strong>Implicit:</strong> SPA (deprecated, use PKCE)</li>
                    </ul>

<pre><code>// Token management for API testing
public class AuthTokenManager {
    private static String cachedToken;
    private static long tokenExpiry;

    public static String getAuthToken() {
        if (cachedToken == null || System.currentTimeMillis() > tokenExpiry) {
            refreshToken();
        }
        return cachedToken;
    }

    private static synchronized void refreshToken() {
        // Client credentials flow for test automation
        Response response = given()
            .contentType("application/x-www-form-urlencoded")
            .formParam("grant_type", "client_credentials")
            .formParam("client_id", Config.getClientId())
            .formParam("client_secret", Config.getClientSecret())
            .formParam("scope", "read:content play:content")
        .when()
            .post(Config.getAuthUrl() + "/oauth/token")
        .then()
            .statusCode(200)
            .extract()
            .response();

        cachedToken = response.path("access_token");
        int expiresIn = response.path("expires_in");
        tokenExpiry = System.currentTimeMillis() + (expiresIn * 1000) - 60000; // 1min buffer
    }
}

// JWT validation testing
@Test
public void testJwtTokenContainsRequiredClaims() {
    String token = AuthTokenManager.getAuthToken();

    // Decode JWT (don't validate signature in test)
    String[] parts = token.split("\\.");
    String payload = new String(Base64.getDecoder().decode(parts[1]));
    JsonObject claims = JsonParser.parseString(payload).getAsJsonObject();

    assertThat(claims.has("sub")).isTrue();
    assertThat(claims.has("exp")).isTrue();
    assertThat(claims.get("iss").getAsString()).isEqualTo("streaming-platform");
    assertThat(claims.getAsJsonArray("scope").toString()).contains("play:content");
}

// Testing authorization rules
@Test
public void testGuestCannotAccessPremiumContent() {
    String guestToken = getGuestToken();

    given()
        .header("Authorization", "Bearer " + guestToken)
    .when()
        .get("/v1/content/premium-movie-123/playback")
    .then()
        .statusCode(403)
        .body("error", equalTo("SUBSCRIPTION_REQUIRED"));
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q25: How do you validate API response schema?</summary>
                <div class="answer">
<pre><code>// JSON Schema validation with REST Assured

// 1. Load schema from file
// src/test/resources/schemas/content-response.json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "required": ["id", "title", "type", "duration"],
  "properties": {
    "id": { "type": "string", "pattern": "^[a-zA-Z0-9-]+$" },
    "title": { "type": "string", "minLength": 1, "maxLength": 200 },
    "type": { "type": "string", "enum": ["MOVIE", "SERIES", "EPISODE"] },
    "duration": { "type": "integer", "minimum": 0 },
    "quality": {
      "type": "array",
      "items": { "type": "string", "enum": ["SD", "HD", "FHD", "4K"] }
    },
    "drm": {
      "type": "object",
      "properties": {
        "type": { "type": "string" },
        "licenseUrl": { "type": "string", "format": "uri" }
      }
    }
  }
}

// Test using schema validation
@Test
public void testContentResponseMatchesSchema() {
    given()
        .spec(authenticatedSpec)
    .when()
        .get("/v1/content/movie-123")
    .then()
        .statusCode(200)
        .body(matchesJsonSchemaInClasspath("schemas/content-response.json"));
}

// Programmatic validation for complex cases
public class SchemaValidator {
    private static JsonSchemaFactory factory = JsonSchemaFactory.getInstance(
        SpecVersion.VersionFlag.V7
    );

    public static void validateResponse(Response response, String schemaPath) {
        JsonSchema schema = factory.getSchema(
            SchemaValidator.class.getResourceAsStream(schemaPath)
        );

        JsonNode responseNode = new ObjectMapper()
            .readTree(response.getBody().asString());

        Set&lt;ValidationMessage&gt; errors = schema.validate(responseNode);

        if (!errors.isEmpty()) {
            throw new AssertionError("Schema validation failed: " + errors);
        }
    }
}

// OpenAPI/Swagger contract testing
// Using Atlassian swagger-request-validator
@Test
public void testApiMatchesOpenApiSpec() {
    OpenApiInteractionValidator validator = OpenApiInteractionValidator
        .createForSpecificationUrl("https://api.streaming.com/openapi.yaml")
        .build();

    Response response = given()
        .spec(authenticatedSpec)
        .get("/v1/content/movie-123");

    ValidationReport report = validator.validate(
        SimpleRequest.Builder.get("/v1/content/movie-123").build(),
        SimpleResponse.Builder.status(response.statusCode())
            .withBody(response.body().asString())
            .build()
    );

    assertThat(report.hasErrors()).isFalse();
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q26: How do you test API performance and response times?</summary>
                <div class="answer">
<pre><code>// REST Assured response time assertions
@Test
public void testSearchApiResponseTime() {
    given()
        .spec(authenticatedSpec)
        .queryParam("q", "action movies")
    .when()
        .get("/v1/search")
    .then()
        .statusCode(200)
        .time(lessThan(2000L)); // Response under 2 seconds
}

// Custom timing measurement
@Test
public void testPlaybackSessionCreationPerformance() {
    long startTime = System.currentTimeMillis();

    Response response = given()
        .spec(authenticatedSpec)
        .body(new PlaybackRequest("movie-123", "WEB"))
        .post("/v1/playback/session");

    long latency = System.currentTimeMillis() - startTime;

    // Attach metrics to report
    Allure.addAttachment("API Latency", String.valueOf(latency) + "ms");

    assertThat(latency).isLessThan(500); // Critical path - must be fast
    assertThat(response.statusCode()).isEqualTo(200);
}

// Load testing concepts (for integration with tools like Gatling/JMeter)
public class PerformanceTestData {
    public static void generateConcurrentRequests(int users, int duration) {
        ExecutorService executor = Executors.newFixedThreadPool(users);
        List&lt;Future&lt;Long&gt;&gt; results = new ArrayList&lt;&gt;();

        long endTime = System.currentTimeMillis() + (duration * 1000);

        for (int i = 0; i < users; i++) {
            results.add(executor.submit(() -> {
                List&lt;Long&gt; responseTimes = new ArrayList&lt;&gt;();
                while (System.currentTimeMillis() < endTime) {
                    long start = System.currentTimeMillis();
                    given().spec(authenticatedSpec)
                        .get("/v1/content/featured");
                    responseTimes.add(System.currentTimeMillis() - start);
                }
                return responseTimes.stream()
                    .mapToLong(Long::longValue)
                    .average()
                    .orElse(0);
            }));
        }

        // Aggregate results
        double avgResponseTime = results.stream()
            .mapToLong(f -> {
                try { return f.get(); }
                catch (Exception e) { return 0; }
            })
            .average()
            .orElse(0);

        assertThat(avgResponseTime).isLessThan(1000);
    }
}

// Percentile-based assertions (p95, p99)
@Test
public void testResponseTimePercentiles() {
    List&lt;Long&gt; responseTimes = new ArrayList&lt;&gt;();

    for (int i = 0; i < 100; i++) {
        long start = System.currentTimeMillis();
        given().spec(authenticatedSpec).get("/v1/catalog");
        responseTimes.add(System.currentTimeMillis() - start);
    }

    Collections.sort(responseTimes);
    long p95 = responseTimes.get(94); // 95th percentile
    long p99 = responseTimes.get(98);

    assertThat(p95).isLessThan(2000);
    assertThat(p99).isLessThan(3000);
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q27: How do you mock external services in API testing?</summary>
                <div class="answer">
<pre><code>// WireMock for service virtualization

// Setup mock server
public class MockServerConfig {
    private static WireMockServer wireMockServer;

    @BeforeClass
    public static void startMockServer() {
        wireMockServer = new WireMockServer(WireMockConfiguration
            .wireMockConfig()
            .port(8089)
            .usingFilesUnderClasspath("wiremock"));
        wireMockServer.start();
        WireMock.configureFor("localhost", 8089);
    }

    @AfterClass
    public static void stopMockServer() {
        wireMockServer.stop();
    }
}

// Mock third-party DRM license server
@Test
public void testPlaybackWithMockedLicenseServer() {
    // Stub DRM license response
    stubFor(post(urlPathEqualTo("/license"))
        .withHeader("Content-Type", containing("octet-stream"))
        .willReturn(aResponse()
            .withStatus(200)
            .withHeader("Content-Type", "application/octet-stream")
            .withBody(loadLicenseResponse())));

    // Test playback flow
    Response playbackResponse = given()
        .spec(authenticatedSpec)
        .body(new PlaybackRequest("drm-content", "WEB"))
        .post("/v1/playback/session");

    // Verify license URL points to mock
    String licenseUrl = playbackResponse.path("licenseUrl");
    assertThat(licenseUrl).contains("localhost:8089");
}

// Mock CDN/streaming errors
@Test
public void testPlaybackHandlesManifestError() {
    stubFor(get(urlPathMatching("/manifest/.*\\.m3u8"))
        .willReturn(aResponse()
            .withStatus(503)
            .withHeader("Retry-After", "30")));

    // Verify application handles gracefully
    given()
        .spec(authenticatedSpec)
    .when()
        .get("/v1/content/movie-123/playback")
    .then()
        .statusCode(200)
        .body("fallbackUrl", notNullValue()); // Should provide fallback
}

// Simulate slow responses
@Test
public void testTimeoutHandling() {
    stubFor(get(urlPathEqualTo("/slow-service"))
        .willReturn(aResponse()
            .withStatus(200)
            .withFixedDelay(5000))); // 5 second delay

    given()
        .config(RestAssuredConfig.config()
            .httpClient(HttpClientConfig.httpClientConfig()
                .setParam("http.connection.timeout", 2000)))
    .when()
        .get("/api-that-calls-slow-service")
    .then()
        .statusCode(504); // Expect gateway timeout
}

// Verify mock was called correctly
@Test
public void testAnalyticsEventSent() {
    stubFor(post(urlPathEqualTo("/analytics/events"))
        .willReturn(aResponse().withStatus(204)));

    // Trigger action that should send analytics
    given().spec(authenticatedSpec)
        .post("/v1/playback/start/movie-123");

    // Verify analytics service was called
    verify(postRequestedFor(urlPathEqualTo("/analytics/events"))
        .withRequestBody(matchingJsonPath("$.event", equalTo("playback_start")))
        .withRequestBody(matchingJsonPath("$.contentId", equalTo("movie-123"))));
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q28: How do you test Kafka message flows?</summary>
                <div class="answer">
<pre><code>// Embedded Kafka for integration tests

@SpringBootTest
@EmbeddedKafka(partitions = 1, topics = {"playback-events", "content-updates"})
public class KafkaIntegrationTest {

    @Autowired
    private EmbeddedKafkaBroker embeddedKafka;

    @Autowired
    private KafkaTemplate&lt;String, String&gt; kafkaTemplate;

    private Consumer&lt;String, String&gt; consumer;

    @BeforeEach
    void setUp() {
        Map&lt;String, Object&gt; consumerProps = KafkaTestUtils.consumerProps(
            "test-group", "true", embeddedKafka);
        consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        consumer = new DefaultKafkaConsumerFactory&lt;&gt;(
            consumerProps,
            new StringDeserializer(),
            new StringDeserializer()
        ).createConsumer();

        consumer.subscribe(Collections.singleton("playback-events"));
    }

    @Test
    void testPlaybackEventPublished() throws Exception {
        // Trigger playback start (this should publish event)
        given()
            .spec(authenticatedSpec)
            .body(new PlaybackRequest("movie-123", "WEB"))
            .post("/v1/playback/session");

        // Consume and verify event
        ConsumerRecords&lt;String, String&gt; records =
            KafkaTestUtils.getRecords(consumer, Duration.ofSeconds(10));

        assertThat(records.count()).isGreaterThan(0);

        ConsumerRecord&lt;String, String&gt; record = records.iterator().next();
        JsonObject event = JsonParser.parseString(record.value()).getAsJsonObject();

        assertThat(event.get("eventType").getAsString()).isEqualTo("PLAYBACK_STARTED");
        assertThat(event.get("contentId").getAsString()).isEqualTo("movie-123");
        assertThat(event.has("timestamp")).isTrue();
    }

    @Test
    void testContentUpdateConsumption() throws Exception {
        // Publish content update event
        String updateEvent = """
            {
                "eventType": "CONTENT_UPDATED",
                "contentId": "movie-456",
                "changes": {"availability": "PREMIUM_ONLY"}
            }
            """;

        kafkaTemplate.send("content-updates", "movie-456", updateEvent);

        // Verify service processed update
        await().atMost(Duration.ofSeconds(10))
            .untilAsserted(() -> {
                Response response = given()
                    .spec(authenticatedSpec)
                    .get("/v1/content/movie-456");

                assertThat(response.path("availability"))
                    .isEqualTo("PREMIUM_ONLY");
            });
    }

    @Test
    void testEventOrdering() {
        // Publish multiple events for same content
        String contentId = "movie-789";

        for (int i = 0; i < 5; i++) {
            kafkaTemplate.send("playback-events", contentId,
                createProgressEvent(contentId, i * 60)); // Every 60 seconds
        }

        // Verify ordering preserved
        ConsumerRecords&lt;String, String&gt; records =
            KafkaTestUtils.getRecords(consumer, Duration.ofSeconds(10));

        int lastProgress = -1;
        for (ConsumerRecord&lt;String, String&gt; record : records) {
            int progress = JsonParser.parseString(record.value())
                .getAsJsonObject()
                .get("progressSeconds")
                .getAsInt();

            assertThat(progress).isGreaterThan(lastProgress);
            lastProgress = progress;
        }
    }
}</code></pre>
                </div>
            </details>

            <h3>CI/CD & DevOps (Questions 29-32)</h3>

            <details>
                <summary>Q29: Describe how you structure test execution in a CI/CD pipeline.</summary>
                <div class="answer">
<pre><code>// Jenkinsfile for streaming platform test pipeline
pipeline {
    agent any

    environment {
        MAVEN_OPTS = '-Xmx2048m'
        TEST_ENV = 'staging'
    }

    parameters {
        choice(name: 'TEST_SUITE',
               choices: ['smoke', 'regression', 'playback', 'api', 'full'],
               description: 'Test suite to run')
        choice(name: 'BROWSER',
               choices: ['chrome', 'firefox', 'all'],
               description: 'Browser for UI tests')
        booleanParam(name: 'PARALLEL', defaultValue: true,
                     description: 'Run tests in parallel')
    }

    stages {
        stage('Build') {
            steps {
                sh 'mvn clean compile -DskipTests'
            }
        }

        stage('Unit Tests') {
            steps {
                sh 'mvn test -Dtest=*UnitTest'
            }
            post {
                always {
                    junit 'target/surefire-reports/*.xml'
                }
            }
        }

        stage('API Tests') {
            parallel {
                stage('Content API') {
                    steps {
                        sh '''
                            mvn test \
                            -Dtest=ContentApiTest \
                            -Denv=${TEST_ENV}
                        '''
                    }
                }
                stage('Playback API') {
                    steps {
                        sh '''
                            mvn test \
                            -Dtest=PlaybackApiTest \
                            -Denv=${TEST_ENV}
                        '''
                    }
                }
            }
        }

        stage('UI Tests') {
            when {
                expression { params.TEST_SUITE in ['regression', 'playback', 'full'] }
            }
            steps {
                sh '''
                    mvn test \
                    -Dsuite=${TEST_SUITE} \
                    -Dbrowser=${BROWSER} \
                                    -Dparallel=${PARALLEL} \
                    -Denv=${TEST_ENV} \
                    -Dselenium.grid.url=${GRID_URL}
                '''
            }
        }

        stage('Cross-Device Tests') {
            when {
                expression { params.TEST_SUITE == 'full' }
            }
            matrix {
                axes {
                    axis {
                        name 'DEVICE'
                        values 'roku', 'firetv', 'appletv', 'smarttv'
                    }
                }
                stages {
                    stage('Device Test') {
                        steps {
                            sh "mvn test -Ddevice=${DEVICE} -Dsuite=device-smoke"
                        }
                    }
                }
            }
        }
    }

    post {
        always {
            allure includeProperties: false,
                   jdk: '',
                   results: [[path: 'target/allure-results']]

            // Archive test artifacts
            archiveArtifacts artifacts: 'target/screenshots/**/*',
                             allowEmptyArchive: true
        }
        failure {
            slackSend channel: '#qa-alerts',
                      color: 'danger',
                      message: "Pipeline FAILED: ${env.JOB_NAME} #${env.BUILD_NUMBER}"
        }
    }
}</code></pre>

                    <p><strong>Test Stage Strategy:</strong></p>
                    <table>
                        <thead>
                            <tr><th>Stage</th><th>Tests</th><th>Frequency</th><th>Blocking</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Smoke</td><td>Critical paths (10-15 tests)</td><td>Every commit</td><td>Yes</td></tr>
                            <tr><td>API</td><td>All backend services</td><td>Every commit</td><td>Yes</td></tr>
                            <tr><td>Regression</td><td>Full UI + Integration</td><td>Nightly</td><td>No</td></tr>
                            <tr><td>Cross-Device</td><td>OTT device coverage</td><td>Weekly / Pre-release</td><td>No</td></tr>
                        </tbody>
                    </table>

                    <div class="best-practice">Keep smoke tests under 10 minutes. Parallelize aggressively. Fail fast on critical paths.</div>
                </div>
            </details>

            <details>
                <summary>Q30: How do you handle test failures and retries in CI pipelines?</summary>
                <div class="answer">
<pre><code>// Jenkins retry mechanisms

// 1. Stage-level retry
stage('Flaky Integration Tests') {
    options {
        retry(2)
    }
    steps {
        sh 'mvn test -Dsuite=integration'
    }
}

// 2. Maven Surefire retry
&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;rerunFailingTestsCount&gt;2&lt;/rerunFailingTestsCount&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;

// 3. TestNG retry analyzer
public class RetryAnalyzer implements IRetryAnalyzer {
    private static final int MAX_RETRY = 2;
    private ThreadLocal&lt;Integer&gt; retryCount = ThreadLocal.withInitial(() -> 0);

    @Override
    public boolean retry(ITestResult result) {
        int count = retryCount.get();
        if (count < MAX_RETRY) {
            retryCount.set(count + 1);

            // Log retry for visibility
            System.out.println("Retrying test: " + result.getName() +
                             " [Attempt " + (count + 2) + "]");

            // Reset driver for clean state
            DriverManager.quitDriver();

            return true;
        }
        retryCount.remove();
        return false;
    }
}

// Apply globally via listener
public class RetryListener implements IAnnotationTransformer {
    @Override
    public void transform(ITestAnnotation annotation, Class testClass,
                         Constructor testConstructor, Method testMethod) {
        annotation.setRetryAnalyzer(RetryAnalyzer.class);
    }
}

// 4. Categorize failures for triage
post {
    failure {
        script {
            def failedTests = readFile('target/surefire-reports/testng-failed.xml')

            if (failedTests.contains('PlaybackTest')) {
                slackSend channel: '#playback-team',
                          message: "Playback tests failing!"
            }

            if (failedTests.contains('ApiTest')) {
                slackSend channel: '#backend-team',
                          message: "API tests failing!"
            }
        }
    }
}</code></pre>

                    <div class="warning">Track retry rates. If a test requires retries more than 5% of runs, it should be fixed or quarantined, not permanently retried.</div>
                </div>
            </details>

            <details>
                <summary>Q31: How do you integrate Docker with your test framework?</summary>
                <div class="answer">
<pre><code>// Docker Compose for test infrastructure
// docker-compose.test.yml

version: '3.8'
services:
  selenium-hub:
    image: selenium/hub:4.15
    ports:
      - "4444:4444"
    environment:
      - GRID_MAX_SESSION=10

  chrome:
    image: selenium/node-chrome:4.15
    depends_on:
      - selenium-hub
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
      - SE_NODE_MAX_SESSIONS=5
    shm_size: '2gb'

  firefox:
    image: selenium/node-firefox:4.15
    depends_on:
      - selenium-hub
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
    shm_size: '2gb'

  test-db:
    image: postgres:14
    environment:
      POSTGRES_DB: test_db
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
    ports:
      - "5432:5432"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
    depends_on:
      - zookeeper

// Jenkinsfile with Docker
pipeline {
    agent {
        docker {
            image 'maven:3.9-eclipse-temurin-17'
            args '-v /var/run/docker.sock:/var/run/docker.sock'
        }
    }

    stages {
        stage('Start Test Infrastructure') {
            steps {
                sh 'docker-compose -f docker-compose.test.yml up -d'
                sh 'sleep 30' // Wait for services
            }
        }

        stage('Run Tests') {
            steps {
                sh '''
                    mvn test \
                    -Dselenium.grid.url=http://selenium-hub:4444 \
                    -Ddb.url=jdbc:postgresql://test-db:5432/test_db \
                    -Dkafka.bootstrap.servers=kafka:9092
                '''
            }
        }
    }

    post {
        always {
            sh 'docker-compose -f docker-compose.test.yml down -v'
        }
    }
}

// Testcontainers for isolated integration tests
public class DatabaseIntegrationTest {
    @Container
    static PostgreSQLContainer&lt;?&gt; postgres = new PostgreSQLContainer&lt;&gt;("postgres:14")
        .withDatabaseName("test")
        .withUsername("test")
        .withPassword("test");

    @Container
    static KafkaContainer kafka = new KafkaContainer(
        DockerImageName.parse("confluentinc/cp-kafka:7.5.0"));

    @BeforeAll
    static void setup() {
        System.setProperty("db.url", postgres.getJdbcUrl());
        System.setProperty("kafka.bootstrap", kafka.getBootstrapServers());
    }
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q32: How do you optimize pipeline execution time?</summary>
                <div class="answer">
                    <p><strong>Optimization Strategies:</strong></p>

                    <ol>
                        <li><strong>Parallel Execution</strong></li>
<pre><code>// TestNG parallel configuration
&lt;suite parallel="classes" thread-count="10"&gt;

// Maven parallel execution
mvn test -T 4 // 4 threads

// Pipeline parallel stages
parallel {
    stage('Chrome') { ... }
    stage('Firefox') { ... }
}</code></pre>

                        <li><strong>Test Selection / Impact Analysis</strong></li>
<pre><code>// Run only tests affected by code changes
stage('Smart Test Selection') {
    steps {
        script {
            def changedFiles = sh(script: 'git diff --name-only HEAD~1',
                                  returnStdout: true)

            if (changedFiles.contains('playback')) {
                sh 'mvn test -Dgroups=playback'
            } else if (changedFiles.contains('api')) {
                sh 'mvn test -Dgroups=api'
            } else {
                sh 'mvn test -Dsuite=smoke'
            }
        }
    }
}</code></pre>

                        <li><strong>Caching Dependencies</strong></li>
<pre><code>// Jenkins caching
options {
    buildDiscarder(logRotator(numToKeepStr: '10'))
}

environment {
    MAVEN_OPTS = '-Dmaven.repo.local=.m2/repository'
}

// GitLab CI caching
cache:
  key: ${CI_COMMIT_REF_SLUG}
  paths:
    - .m2/repository/</code></pre>

                        <li><strong>Fail Fast Strategy</strong></li>
<pre><code>// Stop on first critical failure
stage('Critical Tests') {
    steps {
        sh 'mvn test -Dsuite=smoke -Dsurefire.skipAfterFailureCount=1'
    }
}</code></pre>
                    </ol>

                    <table>
                        <thead>
                            <tr><th>Optimization</th><th>Time Savings</th><th>Trade-off</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Parallel execution (10 threads)</td><td>60-70%</td><td>Infrastructure cost</td></tr>
                            <tr><td>Dependency caching</td><td>2-5 minutes</td><td>Cache invalidation</td></tr>
                            <tr><td>Smart test selection</td><td>40-80%</td><td>Risk of missing regressions</td></tr>
                            <tr><td>Headless browsers</td><td>10-20%</td><td>Miss visual issues</td></tr>
                        </tbody>
                    </table>
                </div>
            </details>

            <h3>Databases & Data Validation (Questions 33-35)</h3>

            <details>
                <summary>Q33: How do you validate data integrity between UI, API, and database?</summary>
                <div class="answer">
<pre><code>// Three-layer validation pattern

public class ContentValidationTest extends BaseTest {

    @Test
    public void testContentDataConsistency() {
        String contentId = "movie-12345";

        // LAYER 1: Database (Source of truth)
        ContentEntity dbContent = jdbcTemplate.queryForObject(
            "SELECT id, title, duration_seconds, status, drm_type " +
            "FROM content WHERE id = ?",
            new ContentRowMapper(),
            contentId
        );

        // LAYER 2: API
        ContentResponse apiContent = given()
            .spec(authenticatedSpec)
            .get("/v1/content/" + contentId)
            .then()
            .extract()
            .as(ContentResponse.class);

        // LAYER 3: UI
        driver.get(baseUrl + "/content/" + contentId);
        ContentDetailsPage page = new ContentDetailsPage(driver);

        // Cross-layer assertions
        SoftAssertions soft = new SoftAssertions();

        // Title consistency
        soft.assertThat(apiContent.getTitle())
            .as("API title should match DB")
            .isEqualTo(dbContent.getTitle());
        soft.assertThat(page.getTitle())
            .as("UI title should match DB")
            .isEqualTo(dbContent.getTitle());

        // Duration consistency
        soft.assertThat(apiContent.getDurationMinutes())
            .as("API duration should match DB")
            .isEqualTo(dbContent.getDurationSeconds() / 60);
        soft.assertThat(page.getDisplayedDuration())
            .as("UI duration should match formatted DB value")
            .isEqualTo(formatDuration(dbContent.getDurationSeconds()));

        // DRM consistency
        soft.assertThat(apiContent.getDrmType())
            .as("DRM type should match")
            .isEqualTo(dbContent.getDrmType());

        soft.assertAll();
    }

    @Test
    public void testWatchProgressSync() {
        String userId = testUser.getId();
        String contentId = "movie-12345";
        int progressSeconds = 600; // 10 minutes

        // Update progress via API
        given()
            .spec(authenticatedSpec)
            .body(Map.of("progressSeconds", progressSeconds))
            .post("/v1/users/{userId}/progress/{contentId}", userId, contentId);

        // Verify in database
        Integer dbProgress = jdbcTemplate.queryForObject(
            "SELECT progress_seconds FROM watch_progress " +
            "WHERE user_id = ? AND content_id = ?",
            Integer.class, userId, contentId
        );
        assertThat(dbProgress).isEqualTo(progressSeconds);

        // Verify in UI (Continue Watching section)
        driver.get(baseUrl + "/home");
        HomePage homePage = new HomePage(driver);

        assertThat(homePage.getContinueWatchingProgress(contentId))
            .isEqualTo("10:00");
    }
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q34: Write a complex SQL query to validate streaming analytics data.</summary>
                <div class="answer">
<pre><code>-- Validate playback session analytics

-- Query: Find discrepancies between playback events and sessions
WITH session_events AS (
    SELECT
        session_id,
        content_id,
        user_id,
        MIN(event_timestamp) as session_start,
        MAX(event_timestamp) as session_end,
        SUM(CASE WHEN event_type = 'BUFFER' THEN duration_ms ELSE 0 END) as total_buffer_time,
        SUM(CASE WHEN event_type = 'PLAY' THEN duration_ms ELSE 0 END) as total_play_time,
        COUNT(CASE WHEN event_type = 'ERROR' THEN 1 END) as error_count,
        MAX(CASE WHEN event_type = 'QUALITY_CHANGE' THEN quality END) as max_quality
    FROM playback_events
    WHERE event_date = CURRENT_DATE - 1
    GROUP BY session_id, content_id, user_id
),
session_summary AS (
    SELECT
        s.session_id,
        s.content_id,
        s.duration_seconds,
        s.completed,
        s.created_at,
        s.ended_at
    FROM playback_sessions s
    WHERE DATE(s.created_at) = CURRENT_DATE - 1
)

SELECT
    se.session_id,
    se.content_id,
    se.total_play_time / 1000 as calculated_duration_seconds,
    ss.duration_seconds as reported_duration_seconds,
    ABS(se.total_play_time / 1000 - ss.duration_seconds) as duration_discrepancy,
    se.total_buffer_time / 1000 as buffer_seconds,
    se.error_count,
    CASE
        WHEN se.total_play_time / 1000 &lt; ss.duration_seconds * 0.9
        THEN 'UNDER_REPORTED'
        WHEN se.total_play_time / 1000 > ss.duration_seconds * 1.1
        THEN 'OVER_REPORTED'
        ELSE 'ACCEPTABLE'
    END as discrepancy_status
FROM session_events se
JOIN session_summary ss ON se.session_id = ss.session_id
WHERE ABS(se.total_play_time / 1000 - ss.duration_seconds) > 30
ORDER BY duration_discrepancy DESC
LIMIT 100;

-- Query: Content performance analysis
SELECT
    c.id as content_id,
    c.title,
    c.genre,
    COUNT(DISTINCT ps.user_id) as unique_viewers,
    COUNT(ps.session_id) as total_sessions,
    AVG(ps.duration_seconds) as avg_watch_time,
    AVG(ps.duration_seconds) / c.duration_seconds * 100 as avg_completion_rate,
    SUM(CASE WHEN ps.completed THEN 1 ELSE 0 END) as completed_views,
    AVG(COALESCE(
        (SELECT SUM(duration_ms) FROM playback_events
         WHERE session_id = ps.session_id AND event_type = 'BUFFER'),
        0
    )) / 1000 as avg_buffer_time_seconds
FROM content c
JOIN playback_sessions ps ON c.id = ps.content_id
WHERE ps.created_at >= CURRENT_DATE - 7
GROUP BY c.id, c.title, c.genre, c.duration_seconds
HAVING COUNT(ps.session_id) >= 100
ORDER BY avg_completion_rate DESC;</code></pre>

                    <p><strong>Test Implementation:</strong></p>
<pre><code>@Test
public void testPlaybackAnalyticsAccuracy() {
    List&lt;Map&lt;String, Object&gt;&gt; discrepancies = jdbcTemplate.queryForList(
        discrepancyQuery,
        LocalDate.now().minusDays(1)
    );

    // Allow up to 1% discrepancy rate
    long totalSessions = getTotalSessionCount();
    assertThat(discrepancies.size())
        .as("Discrepancy count should be under 1%")
        .isLessThan(totalSessions / 100);

    // Log discrepancies for investigation
    discrepancies.forEach(d ->
        Allure.addAttachment("Discrepancy", d.toString()));
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q35: How do you handle test data cleanup and database state management?</summary>
                <div class="answer">
<pre><code>// Strategy 1: Transaction Rollback
@Transactional
@Rollback
public class TransactionalTestBase {
    // All changes rolled back after each test
}

// Strategy 2: Explicit Cleanup with Tags
public class DataCleanupManager {
    private static final String TEST_DATA_TAG = "TEST_AUTO_";

    public User createTestUser() {
        User user = new User();
        user.setEmail(TEST_DATA_TAG + UUID.randomUUID() + "@test.com");
        return userRepository.save(user);
    }

    @AfterSuite
    public void cleanupAllTestData() {
        jdbcTemplate.execute(
            "DELETE FROM watch_progress WHERE user_id IN " +
            "(SELECT id FROM users WHERE email LIKE 'TEST_AUTO_%')"
        );
        jdbcTemplate.execute(
            "DELETE FROM users WHERE email LIKE 'TEST_AUTO_%'"
        );
    }
}

// Strategy 3: Database Snapshots
public class DatabaseSnapshotManager {

    @BeforeSuite
    public void createSnapshot() {
        // PostgreSQL
        jdbcTemplate.execute(
            "SELECT pg_create_restore_point('test_baseline')"
        );
    }

    @AfterSuite
    public void restoreSnapshot() {
        // Restore to baseline
        // In practice, use pg_restore or similar
    }
}

// Strategy 4: Isolated Test Database
// docker-compose.test.yml creates fresh DB per run

// Strategy 5: API-based Cleanup
public class TestDataApi {

    @AfterMethod
    public void cleanupViaApi() {
        // Use admin API to clean test data
        given()
            .header("X-Admin-Key", adminKey)
            .delete("/admin/test-data/user/" + testUserId);
    }
}

// Strategy 6: Soft Assertions with Cleanup
@Test
public void testWithGuaranteedCleanup() {
    String contentId = null;
    try {
        // Create test content
        contentId = createTestContent();

        // Test logic
        performTest(contentId);

    } finally {
        // Always cleanup
        if (contentId != null) {
            deleteTestContent(contentId);
        }
    }
}</code></pre>

                    <div class="best-practice">For streaming platforms, prefer API-based cleanup over direct DB operations. This ensures all caches (CDN, application) are properly invalidated.</div>
                </div>
            </details>

            <h3>Streaming & Playback Testing (Questions 36-40)</h3>

            <details>
                <summary>Q36: Explain HLS and DASH protocols. How do you test adaptive bitrate streaming?</summary>
                <div class="answer">
                    <p><strong>HLS (HTTP Live Streaming):</strong></p>
                    <ul>
                        <li>Apple's protocol, widely supported</li>
                        <li>Uses .m3u8 manifest files and .ts segment files</li>
                        <li>Segment duration typically 2-10 seconds</li>
                    </ul>

                    <p><strong>DASH (Dynamic Adaptive Streaming over HTTP):</strong></p>
                    <ul>
                        <li>MPEG standard, codec agnostic</li>
                        <li>Uses .mpd manifest (XML) and segment files</li>
                        <li>More flexible than HLS</li>
                    </ul>

<pre><code>// HLS Manifest Structure
#EXTM3U
#EXT-X-STREAM-INF:BANDWIDTH=800000,RESOLUTION=640x360
360p/playlist.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=1400000,RESOLUTION=1280x720
720p/playlist.m3u8
#EXT-X-STREAM-INF:BANDWIDTH=3500000,RESOLUTION=1920x1080
1080p/playlist.m3u8

// Testing ABR (Adaptive Bitrate) Switching
public class ABRStreamingTest extends BaseTest {

    @Test
    public void testQualityAdaptation() {
        videoPlayer.loadContent("abr-test-content");
        videoPlayer.play();

        // Start with good network
        networkController.setCondition(NetworkProfile.WIFI);
        waitForStableQuality();
        String initialQuality = videoPlayer.getCurrentQuality();
        assertThat(initialQuality).isIn("1080p", "720p");

        // Throttle network
        networkController.setCondition(NetworkProfile.SLOW_3G);

        // Wait for quality to adapt down
        await().atMost(Duration.ofSeconds(60))
            .until(() -> {
                String quality = videoPlayer.getCurrentQuality();
                return quality.equals("360p") || quality.equals("480p");
            });

        // Restore network
        networkController.setCondition(NetworkProfile.WIFI);

        // Wait for quality to recover
        await().atMost(Duration.ofSeconds(60))
            .until(() -> {
                String quality = videoPlayer.getCurrentQuality();
                return quality.equals("1080p") || quality.equals("720p");
            });
    }

    @Test
    public void testManifestValidity() {
        // Fetch and validate HLS manifest
        Response manifestResponse = given()
            .get(manifestUrl);

        String manifest = manifestResponse.body().asString();

        // Validate structure
        assertThat(manifest).contains("#EXTM3U");
        assertThat(manifest).contains("#EXT-X-STREAM-INF");

        // Extract and validate each variant
        List&lt;String&gt; variantUrls = parseVariantUrls(manifest);

        for (String variantUrl : variantUrls) {
            Response variantResponse = given().get(variantUrl);
            assertThat(variantResponse.statusCode()).isEqualTo(200);

            // Validate segments are accessible
            String variantManifest = variantResponse.body().asString();
            List&lt;String&gt; segmentUrls = parseSegmentUrls(variantManifest);

            // Check first and last segment
            assertThat(given().get(segmentUrls.get(0)).statusCode())
                .isEqualTo(200);
        }
    }
}

// Network throttling with Chrome DevTools Protocol
public class NetworkController {
    private DevTools devTools;

    public void setCondition(NetworkProfile profile) {
        devTools.send(Network.emulateNetworkConditions(
            false, // offline
            profile.getLatency(),
            profile.getDownloadThroughput(),
            profile.getUploadThroughput(),
            Optional.of(ConnectionType.CELLULAR3G)
        ));
    }
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q37: How do you test DRM (Digital Rights Management) protected content?</summary>
                <div class="answer">
                    <p><strong>Common DRM Systems:</strong></p>
                    <ul>
                        <li><strong>Widevine:</strong> Google (Chrome, Android, most browsers)</li>
                        <li><strong>FairPlay:</strong> Apple (Safari, iOS, tvOS)</li>
                        <li><strong>PlayReady:</strong> Microsoft (Edge, Xbox, Windows)</li>
                    </ul>

<pre><code>// DRM Testing Strategy

public class DRMPlaybackTest extends BaseTest {

    @Test
    public void testWidevinePlayback() {
        // Ensure Chrome has Widevine enabled
        ChromeOptions options = new ChromeOptions();
        options.addArguments("--enable-features=WidevineHardwareSecureDecryption");

        driver = new ChromeDriver(options);

        videoPlayer.loadDRMContent("drm-movie-123");
        videoPlayer.play();

        // Verify license acquisition
        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.isDRMLicenseAcquired());

        // Verify playback
        assertThat(videoPlayer.isPlaying()).isTrue();
        assertThat(videoPlayer.getCurrentTime()).isGreaterThan(0);
    }

    @Test
    public void testLicenseServerCommunication() {
        // Intercept and validate license requests
        List&lt;NetworkRequest&gt; licenseRequests = new ArrayList&lt;&gt;();

        devTools.addListener(Network.requestWillBeSent(), request -> {
            if (request.getRequest().getUrl().contains("/license")) {
                licenseRequests.add(request);
            }
        });

        videoPlayer.loadDRMContent("drm-movie-123");
        videoPlayer.play();

        await().atMost(Duration.ofSeconds(30))
            .until(() -> !licenseRequests.isEmpty());

        // Validate license request
        NetworkRequest licenseReq = licenseRequests.get(0);
        assertThat(licenseReq.getRequest().getMethod()).isEqualTo("POST");
        assertThat(licenseReq.getRequest().getHeaders())
            .containsKey("Content-Type");
    }

    @Test
    public void testDRMErrorHandling() {
        // Test expired license scenario
        mockDRMServer.returnExpiredLicense();

        videoPlayer.loadDRMContent("drm-movie-123");
        videoPlayer.play();

        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.hasError());

        assertThat(videoPlayer.getErrorType()).isEqualTo("LICENSE_EXPIRED");
        assertThat(videoPlayer.getErrorMessage())
            .contains("subscription");
    }

    @Test
    @Parameters({"chrome", "firefox", "safari"})
    public void testCrossLibraryDRMSupport(String browser) {
        // Different browsers use different DRM libraries
        setupBrowser(browser);

        String expectedDRM = switch (browser) {
            case "chrome", "firefox" -> "WIDEVINE";
            case "safari" -> "FAIRPLAY";
            default -> throw new IllegalArgumentException();
        };

        videoPlayer.loadDRMContent("drm-movie-123");

        assertThat(videoPlayer.getActiveDRMSystem())
            .isEqualTo(expectedDRM);
    }
}</code></pre>

                    <div class="warning">Never attempt to circumvent or test bypassing DRM protections. Focus on validating correct DRM implementation and error handling.</div>
                </div>
            </details>

            <details>
                <summary>Q38: How do you test video player error scenarios and recovery?</summary>
                <div class="answer">
<pre><code>public class VideoPlayerErrorTest extends BaseTest {

    @Test
    public void testNetworkLossRecovery() {
        videoPlayer.loadContent("test-movie");
        videoPlayer.play();

        // Wait for stable playback
        waitForBufferComplete();
        double playPositionBeforeDisconnect = videoPlayer.getCurrentTime();

        // Simulate network loss
        networkController.goOffline();

        // Wait for buffer to deplete
        await().atMost(Duration.ofSeconds(60))
            .until(() -> videoPlayer.isBuffering() || videoPlayer.hasError());

        // Restore network
        networkController.goOnline();

        // Verify recovery
        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.isPlaying());

        // Verify playback resumed near where it stopped
        assertThat(videoPlayer.getCurrentTime())
            .isCloseTo(playPositionBeforeDisconnect, within(10.0));
    }

    @Test
    public void testManifestError() {
        // Mock manifest to return 404
        mockCDN.returnErrorForManifest(404);

        videoPlayer.loadContent("test-movie");

        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.hasError());

        assertThat(videoPlayer.getErrorCode()).isEqualTo("MANIFEST_LOAD_ERROR");

        // Verify user-friendly error message
        assertThat(videoPlayer.getErrorMessage())
            .doesNotContain("404")
            .contains("trouble playing");
    }

    @Test
    public void testSegmentError() {
        videoPlayer.loadContent("test-movie");
        videoPlayer.play();

        // After playback starts, fail specific segment
        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.getCurrentTime() > 5);

        mockCDN.failSegment(3); // Fail 3rd segment

        // Player should handle gracefully - either skip or retry
        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.getCurrentTime() > 20);

        // Playback should continue
        assertThat(videoPlayer.isPlaying()).isTrue();
    }

    @Test
    public void testConcurrencyLimitError() {
        // Login on max allowed devices
        for (int i = 0; i < 3; i++) {
            loginOnDevice("device-" + i);
            startPlayback("test-movie");
        }

        // Try to play on 4th device (over limit)
        loginOnDevice("device-4");
        videoPlayer.loadContent("test-movie");
        videoPlayer.play();

        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.hasError());

        assertThat(videoPlayer.getErrorCode()).isEqualTo("CONCURRENT_STREAM_LIMIT");
    }

    @Test
    public void testGeoBlockedContent() {
        // Simulate VPN/geo-blocked region
        mockGeoService.setRegion("BLOCKED_REGION");

        videoPlayer.loadContent("region-restricted-movie");

        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.hasError());

        assertThat(videoPlayer.getErrorCode()).isEqualTo("GEO_BLOCKED");
        assertThat(videoPlayer.getErrorMessage())
            .contains("not available in your region");
    }
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q39: How do you test playback performance metrics (buffering, startup time)?</summary>
                <div class="answer">
<pre><code>public class PlaybackPerformanceTest extends BaseTest {

    private PlaybackMetricsCollector metricsCollector;

    @BeforeMethod
    public void setupMetrics() {
        metricsCollector = new PlaybackMetricsCollector(driver);
    }

    @Test
    public void testStartupTime() {
        long loadStartTime = System.currentTimeMillis();

        videoPlayer.loadContent("test-movie");

        // Wait for first frame
        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.getCurrentTime() > 0);

        long startupTime = System.currentTimeMillis() - loadStartTime;

        // Log metrics
        Allure.addAttachment("Startup Time (ms)", String.valueOf(startupTime));

        // Assert performance SLA
        assertThat(startupTime)
            .as("Video startup time should be under 3 seconds")
            .isLessThan(3000);
    }

    @Test
    public void testBufferingRatio() {
        videoPlayer.loadContent("test-movie");
        videoPlayer.play();

        metricsCollector.startCollecting();

        // Play for 2 minutes
        sleep(120_000);

        PlaybackMetrics metrics = metricsCollector.getMetrics();

        double bufferingRatio = (double) metrics.getTotalBufferingTime() /
                               metrics.getTotalPlayTime() * 100;

        Allure.addAttachment("Buffering Ratio",
            String.format("%.2f%%", bufferingRatio));

        assertThat(bufferingRatio)
            .as("Buffering should be under 1% of play time")
            .isLessThan(1.0);
    }

    @Test
    public void testSeekPerformance() {
        videoPlayer.loadContent("test-movie");
        videoPlayer.play();

        waitForPlaybackStable();

        // Measure seek time
        long seekStart = System.currentTimeMillis();
        videoPlayer.seekTo(300); // Seek to 5 minutes

        await().atMost(Duration.ofSeconds(10))
            .until(() -> !videoPlayer.isBuffering());

        long seekTime = System.currentTimeMillis() - seekStart;

        assertThat(seekTime)
            .as("Seek should complete within 2 seconds")
            .isLessThan(2000);
    }
}

// Metrics collector using JavaScript injection
public class PlaybackMetricsCollector {
    private JavascriptExecutor js;

    public void startCollecting() {
        js.executeScript(
            "window.playbackMetrics = {" +
            "  bufferingEvents: []," +
            "  qualityChanges: []," +
            "  errors: []," +
            "  startTime: Date.now()" +
            "};" +
            "var video = document.querySelector('video');" +
            "video.addEventListener('waiting', function() {" +
            "  window.playbackMetrics.bufferingEvents.push({" +
            "    start: Date.now()," +
            "    position: video.currentTime" +
            "  });" +
            "});" +
            "video.addEventListener('playing', function() {" +
            "  var events = window.playbackMetrics.bufferingEvents;" +
            "  if (events.length > 0 && !events[events.length-1].end) {" +
            "    events[events.length-1].end = Date.now();" +
            "  }" +
            "});"
        );
    }

    public PlaybackMetrics getMetrics() {
        Map&lt;String, Object&gt; raw = (Map&lt;String, Object&gt;) js.executeScript(
            "return window.playbackMetrics;"
        );
        return new PlaybackMetrics(raw);
    }
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q40: How do you test ad insertion in video playback (pre-roll, mid-roll)?</summary>
                <div class="answer">
<pre><code>public class AdPlaybackTest extends BaseTest {

    @Test
    public void testPreRollAdPlayback() {
        videoPlayer.loadContentWithAds("ad-supported-movie");

        // Pre-roll should play first
        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.isPlayingAd());

        assertThat(videoPlayer.getAdPosition()).isEqualTo("pre-roll");

        // Verify ad completion
        await().atMost(Duration.ofSeconds(60))
            .until(() -> !videoPlayer.isPlayingAd());

        // Main content should start
        assertThat(videoPlayer.isPlayingMainContent()).isTrue();
        assertThat(videoPlayer.getCurrentTime()).isEqualTo(0);
    }

    @Test
    public void testMidRollAdTrigger() {
        videoPlayer.loadContentWithAds("ad-supported-movie");

        // Skip pre-roll (or wait for it)
        skipAdsIfPresent();

        // Get mid-roll break points
        List&lt;Integer&gt; adBreakPoints = videoPlayer.getAdBreakPoints();
        assertThat(adBreakPoints).isNotEmpty();

        int firstMidRoll = adBreakPoints.get(0);

        // Seek to just before mid-roll
        videoPlayer.seekTo(firstMidRoll - 5);

        // Wait for mid-roll
        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.isPlayingAd());

        assertThat(videoPlayer.getAdPosition()).isEqualTo("mid-roll");

        // After ad, should resume at correct position
        await().atMost(Duration.ofSeconds(60))
            .until(() -> !videoPlayer.isPlayingAd());

        assertThat(videoPlayer.getCurrentTime())
            .isCloseTo(firstMidRoll, within(2.0));
    }

    @Test
    public void testSkipAdButton() {
        videoPlayer.loadContentWithSkippableAd("movie-with-skippable-ads");

        // Wait for ad
        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.isPlayingAd());

        // Skip button should appear after 5 seconds
        assertThat(videoPlayer.isSkipButtonVisible()).isFalse();

        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.isSkipButtonVisible());

        // Click skip
        videoPlayer.clickSkipAd();

        // Should go to content
        await().atMost(Duration.ofSeconds(5))
            .until(() -> !videoPlayer.isPlayingAd());
    }

    @Test
    public void testAdTrackingBeacons() {
        List&lt;String&gt; trackingCalls = new ArrayList&lt;&gt;();

        // Intercept tracking requests
        devTools.addListener(Network.requestWillBeSent(), request -> {
            String url = request.getRequest().getUrl();
            if (url.contains("tracking") || url.contains("beacon")) {
                trackingCalls.add(url);
            }
        });

        videoPlayer.loadContentWithAds("ad-supported-movie");

        // Play through entire ad
        await().atMost(Duration.ofSeconds(60))
            .until(() -> !videoPlayer.isPlayingAd() &&
                        videoPlayer.getCurrentTime() > 0);

        // Verify tracking events fired
        assertThat(trackingCalls).anyMatch(url -> url.contains("impression"));
        assertThat(trackingCalls).anyMatch(url -> url.contains("start"));
        assertThat(trackingCalls).anyMatch(url -> url.contains("complete"));
    }

    @Test
    public void testAdBlockerDetection() {
        // Simulate ad blocker
        enableAdBlocker();

        videoPlayer.loadContentWithAds("ad-supported-movie");

        // Verify ad blocker detection
        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.isAdBlockerDetected());

        // Verify appropriate message shown
        assertThat(videoPlayer.getAdBlockerMessage())
            .contains("disable your ad blocker");
    }
}</code></pre>
                </div>
            </details>

            <h3>Cross-Platform / OTT Testing (Questions 41-43)</h3>

            <details>
                <summary>Q41: What are the unique challenges of testing on OTT devices (Roku, Fire TV, Apple TV)?</summary>
                <div class="answer">
                    <p><strong>Key OTT Testing Challenges:</strong></p>

                    <table>
                        <thead>
                            <tr><th>Challenge</th><th>Details</th><th>Solution</th></tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Input Method</strong></td>
                                <td>Remote control only (D-pad navigation)</td>
                                <td>Focus management testing, keyboard shortcuts</td>
                            </tr>
                            <tr>
                                <td><strong>Limited Automation Tools</strong></td>
                                <td>No WebDriver equivalent</td>
                                <td>Device-specific tools (Roku WebDriver, Fire TV ADB)</td>
                            </tr>
                            <tr>
                                <td><strong>Hardware Fragmentation</strong></td>
                                <td>Different chipsets, memory constraints</td>
                                <td>Real device testing, performance profiling</td>
                            </tr>
                            <tr>
                                <td><strong>10-Foot UI</strong></td>
                                <td>Large text, simplified navigation</td>
                                <td>Focus on readability, minimum touch targets</td>
                            </tr>
                            <tr>
                                <td><strong>Network Variability</strong></td>
                                <td>Typically WiFi-only, variable quality</td>
                                <td>Test under various network conditions</td>
                            </tr>
                        </tbody>
                    </table>

<pre><code>// Roku WebDriver example
public class RokuTest {
    private RokuDriver driver;

    @BeforeMethod
    public void setup() {
        driver = new RokuDriver("192.168.1.100"); // Roku IP
    }

    @Test
    public void testRokuNavigation() {
        // Launch app
        driver.launchApp("dev"); // Development channel

        // Navigate using remote commands
        driver.pressKey(RokuKey.DOWN);
        driver.pressKey(RokuKey.DOWN);
        driver.pressKey(RokuKey.SELECT);

        // Verify screen
        Screenshot screen = driver.getScreenshot();
        assertThat(screen).containsText("Now Playing");
    }
}

// Fire TV using ADB
public class FireTVTest {
    private ADBClient adb;

    @Test
    public void testFireTVPlayback() {
        // Install and launch app
        adb.install("app.apk");
        adb.startActivity("com.streaming.app/.MainActivity");

        // Navigate using key events
        adb.sendKeyEvent(KeyEvent.DPAD_DOWN);
        adb.sendKeyEvent(KeyEvent.DPAD_CENTER); // Select

        // Verify via screenshot + OCR or element inspection
        BufferedImage screen = adb.screenshot();
        assertThat(OCR.extractText(screen)).contains("Playing");
    }
}

// Apple TV using XCUITest
public class AppleTVTest {
    private XCUITestDriver driver;

    @Test
    public void testAppleTVPlayback() {
        // Focus navigation
        driver.findElement(By.name("Browse")).focus();
        driver.pressRemoteButton(RemoteButton.SELECT);

        // Swipe on trackpad
        driver.swipe(Direction.RIGHT);
        driver.pressRemoteButton(RemoteButton.SELECT);

        // Verify playback
        XCUIElement videoView = driver.findElement(By.type("AVPlayerView"));
        assertThat(videoView.isDisplayed()).isTrue();
    }
}</code></pre>
                </div>
            </details>

            <details>
                <summary>Q42: How do you approach cross-device test strategy for a streaming platform?</summary>
                <div class="answer">
<pre><code>// Test Matrix Strategy

/*
Device Categories:
1. HIGH PRIORITY (daily/per-commit)
   - Chrome (Windows, Mac)
   - Safari (Mac, iOS)
   - Chrome Android

2. MEDIUM PRIORITY (nightly)
   - Firefox
   - Edge
   - Fire TV Stick (most common OTT)
   - Roku

3. LOW PRIORITY (weekly)
   - Samsung TV
   - LG TV
   - PlayStation
   - Xbox
   - Older device generations
*/

public class CrossDeviceTestSuite {

    @DataProvider(name = "criticalDevices")
    public Object[][] getCriticalDevices() {
        return new Object[][] {
            {"chrome", "Windows 10", null},
            {"safari", "macOS Monterey", null},
            {"chrome", "Android 13", "Pixel 7"},
            {"safari", "iOS 16", "iPhone 14"}
        };
    }

    @Test(dataProvider = "criticalDevices")
    public void testCorePlaybackFlow(String browser, String os, String device) {
        WebDriver driver = DeviceFactory.create(browser, os, device);

        try {
            // Critical user journey
            HomePage home = new HomePage(driver);
            home.login(testUser);

            SearchPage search = home.search("Stranger Things");
            ContentPage content = search.selectFirstResult();

            VideoPlayer player = content.play();
            player.waitForPlayback();

            assertThat(player.isPlaying()).isTrue();
            assertThat(player.getCurrentQuality()).isIn("720p", "1080p");

        } finally {
            driver.quit();
        }
    }
}

// Cloud Device Farm Integration (BrowserStack/Sauce Labs)
public class CloudDeviceFactory {

    public static WebDriver create(DeviceConfig config) {
        MutableCapabilities caps = new MutableCapabilities();

        // BrowserStack configuration
        caps.setCapability("bstack:options", Map.of(
            "os", config.getOs(),
            "osVersion", config.getOsVersion(),
            "browserName", config.getBrowser(),
            "browserVersion", config.getBrowserVersion(),
            "deviceName", config.getDevice(),
            "realMobile", true,
            "video", true,
            "networkLogs", true
        ));

        return new RemoteWebDriver(
            new URL("https://hub-cloud.browserstack.com/wd/hub"),
            caps
        );
    }
}

// Prioritization matrix
/*
| Test Type          | Web | Mobile Web | Mobile App | Smart TV | Gaming Console |
|--------------------|-----|------------|------------|----------|----------------|
| Login              | ✓   | ✓          | ✓          | ✓        | Weekly         |
| Playback Start     | ✓   | ✓          | ✓          | ✓        | Weekly         |
| Seek               | ✓   | ✓          | ✓          | ✓        | Weekly         |
| Quality Switch     | ✓   | Nightly    | Nightly    | Weekly   | Monthly        |
| Subtitles          | ✓   | ✓          | ✓          | ✓        | Weekly         |
| Audio Tracks       | ✓   | Nightly    | Nightly    | Weekly   | Monthly        |
| DRM                | ✓   | ✓          | ✓          | ✓        | Weekly         |
| Offline Download   | N/A | ✓          | ✓          | N/A      | N/A            |
| Cast               | ✓   | ✓          | ✓          | N/A      | N/A            |
*/</code></pre>
                </div>
            </details>

            <details>
                <summary>Q43: How do you test casting functionality (Chromecast, AirPlay)?</summary>
                <div class="answer">
<pre><code>public class CastingTest extends BaseTest {

    @Test
    public void testChromecastDiscovery() {
        videoPlayer.loadContent("test-movie");

        // Open cast dialog
        videoPlayer.openCastDialog();

        // Wait for device discovery
        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.getAvailableCastDevices().size() > 0);

        List&lt;CastDevice&gt; devices = videoPlayer.getAvailableCastDevices();

        assertThat(devices)
            .extracting(CastDevice::getName)
            .contains("Living Room TV");
    }

    @Test
    public void testChromecastPlayback() {
        // Start playback locally
        videoPlayer.loadContent("test-movie");
        videoPlayer.play();

        double localPosition = waitForPlaybackProgress(10);

        // Cast to device
        videoPlayer.castToDevice("Living Room TV");

        // Verify casting state
        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.isCasting());

        // Verify position carried over
        double castPosition = videoPlayer.getCastPosition();
        assertThat(castPosition)
            .isCloseTo(localPosition, within(5.0));

        // Control from sender
        videoPlayer.remotePause();

        await().atMost(Duration.ofSeconds(5))
            .until(() -> videoPlayer.isRemotePaused());

        // Disconnect
        videoPlayer.stopCasting();

        // Playback should resume locally
        assertThat(videoPlayer.isCasting()).isFalse();
        assertThat(videoPlayer.isPlayingLocally()).isTrue();
    }

    @Test
    public void testAirPlayIntegration() {
        // iOS Safari specific
        videoPlayer.loadContent("test-movie");
        videoPlayer.play();

        // Access AirPlay button
        WebElement airplayButton = driver.findElement(
            By.cssSelector("video::-webkit-media-controls-start-playback-button")
        );

        // Note: Actual AirPlay device selection requires native interaction
        // For automation, verify button is present and accessible
        assertThat(airplayButton.isDisplayed()).isTrue();
    }

    @Test
    public void testCastErrorRecovery() {
        videoPlayer.loadContent("test-movie");
        videoPlayer.play();

        // Cast to device
        videoPlayer.castToDevice("Living Room TV");

        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.isCasting());

        // Simulate cast device going offline
        mockCastReceiver.goOffline();

        // Verify error handling
        await().atMost(Duration.ofSeconds(30))
            .until(() -> videoPlayer.hasCastError());

        // Verify graceful fallback to local playback
        await().atMost(Duration.ofSeconds(10))
            .until(() -> videoPlayer.isPlayingLocally());

        assertThat(videoPlayer.getCurrentTime())
            .as("Should resume near last cast position")
            .isGreaterThan(0);
    }
}</code></pre>

                    <div class="best-practice">Full cast testing requires physical devices. For CI, focus on cast dialog display, device discovery API, and error handling. Use device labs for end-to-end cast testing.</div>
                </div>
            </details>
        </div>
    </section>

    <!-- SECTION 4: HANDS-ON / DESIGN QUESTIONS -->
    <section>
        <h2><span class="emoji">4️⃣</span> Hands-On / Design Questions</h2>
        <div class="section-content">

            <details>
                <summary>Design a test automation framework for a streaming platform</summary>
                <div class="answer">
                    <h4>High-Level Architecture</h4>

<pre><code>┌─────────────────────────────────────────────────────────────────┐
│                        TEST EXECUTION                            │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │  Smoke Suite │  │ Regression   │  │  Device      │          │
│  │  (Per Commit)│  │ (Nightly)    │  │  Specific    │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
└─────────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────────┐
│                        TEST LAYER                                │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Test Classes (PlaybackTests, SearchTests, AuthTests)    │  │
│  └──────────────────────────────────────────────────────────┘  │
│  ┌─────────────────┐  ┌─────────────────┐                      │
│  │  Test Data      │  │  Assertions     │                      │
│  │  Factories      │  │  (Custom)       │                      │
│  └─────────────────┘  └─────────────────┘                      │
└─────────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────────┐
│                    PAGE OBJECT LAYER                             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │  Web Pages   │  │  Mobile      │  │  TV/OTT      │          │
│  │  ├─HomePage  │  │  Pages       │  │  Screens     │          │
│  │  ├─Player    │  │              │  │              │          │
│  │  └─Search    │  │              │  │              │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  Shared Components (NavigationMenu, VideoControls)       │  │
│  └──────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────────┐
│                     API CLIENT LAYER                             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐          │
│  │  Content API │  │  Auth API    │  │  Playback    │          │
│  │  Client      │  │  Client      │  │  API Client  │          │
│  └──────────────┘  └──────────────┘  └──────────────┘          │
└─────────────────────────────────────────────────────────────────┘
                              │
┌─────────────────────────────────────────────────────────────────┐
│                       CORE LAYER                                 │
│  ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐  │
│  │  Driver    │ │  Config    │ │  Reporting │ │  Utilities │  │
│  │  Factory   │ │  Manager   │ │  (Allure)  │ │  (Waits)   │  │
│  └────────────┘ └────────────┘ └────────────┘ └────────────┘  │
└─────────────────────────────────────────────────────────────────┘</code></pre>

                    <h4>Key Design Decisions</h4>

                    <ol>
                        <li><strong>Multi-Platform Support:</strong>
<pre><code>public interface VideoPlayer {
    void play();
    void pause();
    void seekTo(int seconds);
    boolean isPlaying();
}

public class WebVideoPlayer implements VideoPlayer { }
public class MobileVideoPlayer implements VideoPlayer { }
public class TVVideoPlayer implements VideoPlayer { }</code></pre>
                        </li>

                        <li><strong>API + UI Test Sync:</strong>
<pre><code>// Use API for fast state setup
@BeforeMethod
public void setupTestData() {
    testUser = UserApi.createPremiumUser();
    testContent = ContentApi.getAvailableContent();
}

// UI tests focus on user journey
@Test
public void testPlayback() {
    loginPage.login(testUser);
    searchPage.search(testContent.getTitle())
              .selectFirstResult()
              .play();
}</code></pre>
                        </li>

                        <li><strong>Streaming-Specific Utilities:</strong>
<pre><code>public class StreamingTestUtils {
    public void waitForVideoToBuffer();
    public void validateManifest(String url);
    public void assertQualityLevel(String expected);
    public void measureStartupTime();
    public Map&lt;String, Object&gt; capturePlaybackMetrics();
}</code></pre>
                        </li>

                        <li><strong>Device Farm Integration:</strong>
<pre><code>public class DeviceFactory {
    public static Driver create(DeviceType type) {
        return switch(type) {
            case WEB -> new SeleniumWebDriver();
            case MOBILE_WEB -> new AppiumWebDriver();
            case MOBILE_APP -> new AppiumNativeDriver();
            case ROKU -> new RokuDriver();
            case FIRE_TV -> new ADBDriver();
            case APPLE_TV -> new XCUITestDriver();
        };
    }
}</code></pre>
                        </li>
                    </ol>

                    <h4>CI/CD Integration</h4>
<pre><code>// Tiered test execution
Commit → Smoke Tests (5 min) → API Tests (10 min)
         ↓
Nightly → Full Regression → Cross-Browser → Performance
         ↓
Weekly → OTT Devices → Full Device Matrix</code></pre>
                </div>
            </details>

            <details>
                <summary>How would you test video playback on Smart TVs?</summary>
                <div class="answer">
                    <h4>Testing Approach</h4>

                    <p><strong>1. Identify Test Scope:</strong></p>
                    <ul>
                        <li>Playback controls (play, pause, seek, FF/RW)</li>
                        <li>Quality adaptation on limited bandwidth</li>
                        <li>Remote control navigation</li>
                        <li>Subtitle rendering</li>
                        <li>Audio track switching</li>
                        <li>Memory management (long playback sessions)</li>
                        <li>Resume functionality</li>
                        <li>DRM playback (Widevine L1/L3)</li>
                    </ul>

                    <p><strong>2. Automation Strategy:</strong></p>
<pre><code>// Smart TV test automation is limited
// Approach: Hybrid automation + manual

// Automated (where tooling exists):
- Samsung Tizen: Selenium-based WebDriver
- LG webOS: Similar web-based approach
- Android TV: Appium/ADB

// Semi-automated:
- Network-level validation (Charles Proxy)
- Log parsing and validation
- Performance metrics collection

// Manual (test labs):
- Visual quality verification
- Remote control ergonomics
- Voice search</code></pre>

                    <p><strong>3. Test Infrastructure:</strong></p>
<pre><code>// Network setup for testing
┌─────────┐     ┌─────────────┐     ┌────────────┐
│ Smart   │────▶│ Network     │────▶│ CDN/Server │
│ TV      │     │ Simulator   │     │            │
└─────────┘     └─────────────┘     └────────────┘
     │                │
     └────────────────┴────────────────┐
                                       │
                              ┌────────▼────────┐
                              │ Test Controller │
                              │ (Log collection,│
                              │  screenshots)   │
                              └─────────────────┘</code></pre>

                    <p><strong>4. Critical Test Cases:</strong></p>
<pre><code>@Test
public void testLongPlaybackSession() {
    // Smart TVs have limited memory
    // Test for memory leaks over extended playback

    tvPlayer.loadContent("2-hour-movie");
    tvPlayer.play();

    for (int hour = 0; hour < 2; hour++) {
        sleep(Duration.ofHours(1));

        // Check memory
        long memoryUsage = tvDriver.getMemoryUsage();
        assertThat(memoryUsage).isLessThan(MAX_MEMORY_MB);

        // Verify still playing
        assertThat(tvPlayer.isPlaying()).isTrue();
    }
}

@Test
public void testRemoteControlNavigation() {
    // D-pad navigation is critical for TV

    tvDriver.pressKey(TVKey.DOWN); // Navigate to content
    tvDriver.pressKey(TVKey.DOWN);
    tvDriver.pressKey(TVKey.SELECT); // Select

    await().until(() -> tvPlayer.isLoaded());

    tvDriver.pressKey(TVKey.PLAY); // Play

    assertThat(tvPlayer.isPlaying()).isTrue();

    // Verify all controls work
    tvDriver.pressKey(TVKey.PAUSE);
    assertThat(tvPlayer.isPaused()).isTrue();

    tvDriver.pressKey(TVKey.FF);
    assertThat(tvPlayer.getCurrentTime()).isGreaterThan(0);
}</code></pre>

                    <p><strong>5. Challenges & Mitigations:</strong></p>
                    <table>
                        <tr><th>Challenge</th><th>Mitigation</th></tr>
                        <tr><td>No standard automation protocol</td><td>Use manufacturer-specific tools + ADB where possible</td></tr>
                        <tr><td>Physical device management</td><td>Device lab with remote access, cloud device farms (Perfecto)</td></tr>
                        <tr><td>Slow test execution</td><td>Prioritize critical paths, parallelize across devices</td></tr>
                        <tr><td>Limited debugging</td><td>Comprehensive logging, video recording of test runs</td></tr>
                    </table>
                </div>
            </details>

            <details>
                <summary>How would you validate adaptive bitrate streaming (ABR)?</summary>
                <div class="answer">
                    <h4>ABR Validation Strategy</h4>

                    <p><strong>1. Manifest Validation:</strong></p>
<pre><code>@Test
public void testHLSManifestStructure() {
    String manifestContent = fetchManifest(contentId);

    // Parse master playlist
    HLSParser parser = new HLSParser(manifestContent);
    List&lt;Variant&gt; variants = parser.getVariants();

    // Validate all quality levels present
    assertThat(variants)
        .extracting(Variant::getResolution)
        .containsExactlyInAnyOrder("1920x1080", "1280x720", "854x480", "640x360");

    // Validate bandwidth values are accurate
    for (Variant variant : variants) {
        Response segmentResponse = fetchFirstSegment(variant.getPlaylistUrl());

        long actualBitrate = calculateActualBitrate(segmentResponse);
        long declaredBitrate = variant.getBandwidth();

        // Actual should be within 20% of declared
        assertThat(actualBitrate)
            .isCloseTo(declaredBitrate, withinPercentage(20));
    }
}</code></pre>

                    <p><strong>2. Adaptation Logic Testing:</strong></p>
<pre><code>@Test
public void testUpwardQualityAdaptation() {
    // Start with constrained bandwidth
    networkController.setThroughput(1_000_000); // 1 Mbps

    videoPlayer.loadContent("abr-test");
    videoPlayer.play();

    // Should start at low quality
    await().atMost(Duration.ofSeconds(30))
        .until(() -> videoPlayer.getCurrentQualityHeight() <= 480);

    // Improve bandwidth
    networkController.setThroughput(10_000_000); // 10 Mbps

    // Quality should improve
    await().atMost(Duration.ofSeconds(60))
        .until(() -> videoPlayer.getCurrentQualityHeight() >= 1080);
}

@Test
public void testQualityStability() {
    // Stable network should not cause constant switching
    networkController.setThroughput(5_000_000); // 5 Mbps - stable

    videoPlayer.loadContent("abr-test");
    videoPlayer.play();

    // Wait for quality to stabilize
    sleep(30_000);

    List&lt;Integer&gt; qualityChanges = new ArrayList&lt;&gt;();
    int lastQuality = videoPlayer.getCurrentQualityHeight();

    // Monitor for 2 minutes
    for (int i = 0; i < 24; i++) {
        sleep(5_000);
        int currentQuality = videoPlayer.getCurrentQualityHeight();
        if (currentQuality != lastQuality) {
            qualityChanges.add(currentQuality);
            lastQuality = currentQuality;
        }
    }

    // Should not oscillate frequently
    assertThat(qualityChanges.size())
        .as("Quality should be stable on stable network")
        .isLessThan(3);
}</code></pre>

                    <p><strong>3. Buffer Health Validation:</strong></p>
<pre><code>@Test
public void testBufferMaintenance() {
    videoPlayer.loadContent("abr-test");
    videoPlayer.play();

    // Monitor buffer health
    List&lt;Double&gt; bufferLevels = new ArrayList&lt;&gt;();

    for (int i = 0; i < 60; i++) { // Monitor for 5 minutes
        sleep(5_000);
        bufferLevels.add(videoPlayer.getBufferLevelSeconds());
    }

    // Buffer should stay above threshold
    assertThat(bufferLevels)
        .as("Buffer should not drop below 5 seconds")
        .allMatch(b -> b >= 5.0);

    // Buffer should not grow excessively (wastes bandwidth)
    assertThat(bufferLevels)
        .as("Buffer should not exceed 60 seconds")
        .allMatch(b -> b <= 60.0);
}</code></pre>

                    <p><strong>4. Network Simulation Scenarios:</strong></p>
                    <table>
                        <tr><th>Scenario</th><th>Network Profile</th><th>Expected Behavior</th></tr>
                        <tr><td>Stable High BW</td><td>20 Mbps constant</td><td>1080p stable</td></tr>
                        <tr><td>Stable Low BW</td><td>500 Kbps constant</td><td>360p stable</td></tr>
                        <tr><td>Degradation</td><td>10 Mbps → 500 Kbps</td><td>Graceful drop to 360p</td></tr>
                        <tr><td>Improvement</td><td>500 Kbps → 10 Mbps</td><td>Gradual rise to 1080p</td></tr>
                        <tr><td>Fluctuating</td><td>Variable 1-5 Mbps</td><td>Minimal rebuffering</td></tr>
                        <tr><td>Packet Loss</td><td>5 Mbps + 2% loss</td><td>Maintains playback</td></tr>
                    </table>
                </div>
            </details>
        </div>
    </section>

    <!-- SECTION 5: BEHAVIORAL QUESTIONS -->
    <section>
        <h2><span class="emoji">5️⃣</span> Behavioral Questions with STAR-Method Answers</h2>
        <div class="section-content">

            <details>
                <summary>B1: Tell me about a time you dealt with flaky tests that were blocking releases.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        At my previous healthcare company, our nightly regression suite had a 30% failure rate due to flaky tests. Release approvals were consistently delayed as engineers wasted hours investigating false positives.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        I was tasked with reducing flaky test failures to under 5% within one month while maintaining test coverage.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Implemented a flaky test detection system that tracked pass/fail rates per test over 30 days</li>
                            <li>Quarantined the top 20 flaky tests into a separate suite</li>
                            <li>Analyzed root causes: 60% were timing issues, 30% were test data conflicts, 10% were environment issues</li>
                            <li>Refactored tests with explicit waits instead of Thread.sleep()</li>
                            <li>Introduced test data isolation using unique identifiers per test run</li>
                            <li>Added retry mechanism with maximum 2 retries for infrastructure-related failures</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        Reduced flaky failure rate from 30% to 3% within 3 weeks. Release cycles shortened by 4 hours on average. Team confidence in test results improved significantly, and developers started trusting the pipeline again.
                    </div>
                </div>
            </details>

            <details>
                <summary>B2: Describe a situation where you disagreed with a developer about a bug's priority.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        I identified a bug where session tokens weren't being refreshed properly, causing users to be logged out after 2 hours. The developer marked it as low priority since "users can just log in again."
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        I needed to advocate for proper prioritization without creating conflict, as this was a senior developer with significant influence.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Gathered data: Pulled analytics showing 15% of users abandoned sessions after forced logout</li>
                            <li>Calculated business impact: Estimated $50K/month in lost engagement based on subscription churn data</li>
                            <li>Documented user complaints from support tickets mentioning this issue</li>
                            <li>Presented findings in a private 1:1 rather than in a public meeting to avoid putting the developer on the defensive</li>
                            <li>Proposed a collaborative root cause analysis session</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        The developer agreed to reprioritize after seeing the data. The bug was fixed in the next sprint. More importantly, we established a process for impact-based prioritization that prevented similar disagreements.
                    </div>
                </div>
            </details>

            <details>
                <summary>B3: Tell me about a time you mentored a junior QA engineer.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        A new junior QA joined our team with manual testing experience but no automation background. She was struggling with Java basics and felt overwhelmed by our Selenium framework.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Help her become productive in automation within 3 months while continuing my own deliverables.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Created a structured 12-week learning plan with clear milestones</li>
                            <li>Held 30-minute daily sessions covering Java fundamentals, then Selenium concepts</li>
                            <li>Assigned progressively complex tasks: first reading existing tests, then modifying them, then writing new ones</li>
                            <li>Did pair programming on real tickets, letting her drive while I guided</li>
                            <li>Encouraged her to present her work in team meetings to build confidence</li>
                            <li>Provided constructive code review feedback with explanations of best practices</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        Within 3 months, she was independently writing UI automation tests. Within 6 months, she had contributed to our API test framework. She later became the primary owner of our mobile test suite. The mentorship also improved my own skills in explaining concepts clearly.
                    </div>
                </div>
            </details>

            <details>
                <summary>B4: Describe a time you improved the quality culture in your organization.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        At the consulting firm, QA was treated as a gate at the end of sprints. Developers would throw features "over the wall" to QA with 2 days left, making thorough testing impossible.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Shift left to involve QA earlier and make quality a shared responsibility.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Proposed and got approval for QA involvement in sprint planning and story refinement</li>
                            <li>Introduced "Definition of Ready" requiring testable acceptance criteria before development</li>
                            <li>Created a testing workshop for developers covering unit testing best practices</li>
                            <li>Set up a dashboard showing code coverage and test automation metrics visible to everyone</li>
                            <li>Started a "Bug Bash" tradition where entire team tested before releases</li>
                            <li>Celebrated quality wins in sprint retros, not just feature delivery</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        Bug escape rate to production dropped by 40% over 6 months. Developers started writing more unit tests proactively. Sprint-end crunch reduced significantly as issues were caught earlier. The team started referring to quality as "our responsibility" rather than "QA's job."
                    </div>
                </div>
            </details>

            <details>
                <summary>B5: Tell me about a production defect you missed. How did you handle it?</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        After a release, users reported that password reset emails contained broken links. The issue only occurred when special characters were in the email address, which our test data didn't cover.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Address the immediate issue and prevent similar gaps in the future.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Immediately acknowledged the gap in our test coverage without making excuses</li>
                            <li>Created a hotfix test case and validated the fix in staging</li>
                            <li>Conducted a blameless post-mortem with the team</li>
                            <li>Analyzed our test data strategy and found it was too "clean" - didn't include edge cases</li>
                            <li>Implemented a test data generator that included special characters, unicode, and boundary cases</li>
                            <li>Added email validation to our API contract tests</li>
                            <li>Created a checklist for "edge case coverage" to use during test planning</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        The defect was fixed and deployed within 24 hours. The test data improvements we made caught 3 similar issues in subsequent releases before they reached production. I presented the learnings to the broader QA team as a case study.
                    </div>
                </div>
            </details>

            <details>
                <summary>B6: Describe a time you had to learn a new technology quickly to meet a deadline.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        Our team was integrating with Kafka for real-time event streaming, and we had 3 weeks to build integration tests. No one on the QA team had Kafka experience.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Learn Kafka and build a testing strategy before the feature release.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Spent the first 3 days on intensive learning: Kafka documentation, Confluent tutorials, and a Udemy course</li>
                            <li>Set up a local Kafka environment using Docker to experiment</li>
                            <li>Collaborated with the backend developer who was implementing Kafka - asked to pair program</li>
                            <li>Created a spike to test basic producer/consumer functionality</li>
                            <li>Built a test utility using Embedded Kafka for integration tests</li>
                            <li>Documented the testing approach for future team members</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        Delivered working Kafka integration tests before the deadline. The tests caught a serialization bug that would have corrupted production data. My documentation became the team's reference guide for Kafka testing.
                    </div>
                </div>
            </details>

            <details>
                <summary>B7: Tell me about a time you had to push back on a deadline.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        Management wanted to release a major feature update on Friday before a holiday weekend. Testing had revealed several medium-severity bugs, and we hadn't completed performance testing.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Communicate the risk and advocate for quality without being seen as obstructionist.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Prepared a risk assessment document quantifying the issues found</li>
                            <li>Highlighted that a Friday release before a holiday meant no engineers available if issues arose</li>
                            <li>Proposed alternatives: Monday release (2 days later) or limited rollout to 10% of users</li>
                            <li>Presented to stakeholders with data, not emotions</li>
                            <li>Made clear I wasn't blocking but raising concerns for informed decision-making</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        Leadership appreciated the transparent risk assessment. We agreed to a Monday release with a staged rollout. One of the bugs we identified would have affected payment processing - catching it saved significant customer impact and potential revenue loss.
                    </div>
                </div>
            </details>

            <details>
                <summary>B8: Describe your approach to prioritizing test cases when time is limited.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        We had a critical patch release with only 4 hours for testing, but our full regression suite took 8 hours.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Develop a risk-based testing approach that maximized coverage within the time constraint.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Analyzed the code changes to identify affected components</li>
                            <li>Categorized tests into tiers:
                                <ul>
                                    <li>Tier 1: Directly impacted areas (must test)</li>
                                    <li>Tier 2: Integration points with changed code</li>
                                    <li>Tier 3: Unrelated but high-business-value</li>
                                </ul>
                            </li>
                            <li>Ran Tier 1 and 2 tests fully, sampled Tier 3</li>
                            <li>Used API tests for faster validation where UI wasn't essential</li>
                            <li>Documented what was tested vs. what was risk-accepted</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        Completed meaningful testing in 3.5 hours. Found and fixed one critical bug in an integration point. The approach became our standard for hotfix testing, and I later formalized it into a "Risk-Based Testing Playbook."
                    </div>
                </div>
            </details>

            <details>
                <summary>B9: Tell me about a time you automated a previously manual process.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        Our team spent 4 hours every week manually generating test reports by copying data from TestNG results into Excel spreadsheets for management.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Eliminate this manual effort while providing better insights to stakeholders.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Identified requirements: what metrics did management actually need?</li>
                            <li>Implemented Allure reporting with custom categories for business-relevant grouping</li>
                            <li>Created a Jenkins post-build step that published reports to a shared location</li>
                            <li>Added trend analysis showing test results over time</li>
                            <li>Set up Slack notifications with summary metrics after each run</li>
                            <li>Created a dashboard in Grafana pulling data from test execution database</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        Eliminated 4 hours of manual work weekly (200+ hours annually). Reports were now available within minutes of test completion instead of the next day. Management had real-time visibility into quality metrics, and the historical trends helped us identify patterns.
                    </div>
                </div>
            </details>

            <details>
                <summary>B10: Describe a situation where you had to work with an underperforming team member.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        A team member consistently delivered tests late, and the code quality was below standards. Other team members were frustrated, and the manager asked for my input.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Help improve the situation while maintaining team morale and respecting the individual.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Had a private, empathetic conversation to understand any underlying issues</li>
                            <li>Discovered they were struggling with the framework architecture and felt too embarrassed to ask for help</li>
                            <li>Offered to do code reviews before they submitted PRs (early feedback)</li>
                            <li>Broke down their tickets into smaller, more manageable pieces</li>
                            <li>Created internal documentation for common framework patterns they could reference</li>
                            <li>Celebrated small wins publicly to rebuild their confidence</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        Within 2 months, their delivery improved significantly. They started contributing to code reviews themselves. The experience taught me that "underperformance" often has fixable root causes, and supportive intervention beats criticism.
                    </div>
                </div>
            </details>

            <details>
                <summary>B11: Tell me about your biggest failure in test automation and what you learned.</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        Early in my career, I built an automation framework that was tightly coupled to the UI structure. When the front-end team did a major redesign, over 80% of our tests broke.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Recover from the situation and prevent it from happening again.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Accepted responsibility without blaming the front-end team for the redesign</li>
                            <li>Worked overtime to fix critical tests before release</li>
                            <li>Conducted a thorough analysis of why the framework was so fragile</li>
                            <li>Refactored to use data-testid attributes instead of CSS selectors</li>
                            <li>Implemented Page Object Model properly with abstraction layers</li>
                            <li>Added a contract with the front-end team to maintain test-specific attributes</li>
                            <li>Introduced component-based page objects that mirrored their component architecture</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        The next major UI redesign only broke 10% of tests. I learned that maintainability should be prioritized over quick implementation, and that cross-team collaboration on testability is essential.
                    </div>
                </div>
            </details>

            <details>
                <summary>B12: How do you stay current with testing tools and best practices?</summary>
                <div class="answer">
                    <div class="star-section">
                        <span class="star-label">Situation</span>
                        The testing landscape evolves rapidly, and staying stagnant means falling behind.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Task</span>
                        Continuously learn while balancing the demands of daily work.
                    </div>
                    <div class="star-section">
                        <span class="star-label">Action</span>
                        <ol>
                            <li>Subscribe to newsletters: Software Testing Weekly, Ministry of Testing</li>
                            <li>Follow key voices on LinkedIn/Twitter: Angie Jones, Alan Richardson, Jason Arbon</li>
                            <li>Allocate 2-3 hours weekly for learning (often early mornings)</li>
                            <li>Attend virtual conferences: SeleniumConf, Test Automation Summit</li>
                            <li>Experiment with new tools in side projects before proposing for work</li>
                            <li>Participate in QA communities: Reddit r/QualityAssurance, Slack groups</li>
                            <li>Recently completed: Playwright certification, Kafka fundamentals course</li>
                        </ol>
                    </div>
                    <div class="star-section">
                        <span class="star-label">Result</span>
                        This approach helped me introduce Playwright to my team when it became mature enough, improving our test reliability. I've also shared learnings through internal tech talks, establishing myself as a go-to resource for new testing approaches.
                    </div>
                </div>
            </details>
        </div>
    </section>

    <!-- SECTION 6: SCENARIO-BASED QUESTIONS -->
    <section>
        <h2><span class="emoji">6️⃣</span> Scenario-Based Questions (Real Interview Style)</h2>
        <div class="section-content">

            <details>
                <summary>S1: CI pipeline is failing intermittently. How do you investigate?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Assess systematic debugging approach, understanding of CI/CD, and ability to identify root causes vs. symptoms.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Gather Data:</strong>
                            <ul>
                                <li>Pull failure history from last 30 builds</li>
                                <li>Identify patterns: same tests? same time of day? same stage?</li>
                                <li>Check if failures correlate with specific code changes</li>
                            </ul>
                        </li>
                        <li><strong>Categorize Failures:</strong>
                            <ul>
                                <li>Test failures (actual test code)</li>
                                <li>Infrastructure failures (timeout, connection issues)</li>
                                <li>Environment failures (service unavailable, DB issues)</li>
                            </ul>
                        </li>
                        <li><strong>Common Causes for Streaming Platforms:</strong>
                            <ul>
                                <li>CDN caching causing stale manifests</li>
                                <li>Video encoding jobs still running when tests start</li>
                                <li>Concurrent test data conflicts</li>
                                <li>Network latency to cloud services</li>
                            </ul>
                        </li>
                        <li><strong>Investigation Steps:</strong>
<pre><code>// Add detailed logging to failing tests
- Capture screenshots at multiple points
- Log network requests/responses
- Record video player state at failure

// Check infrastructure
- Review Jenkins agent resource usage
- Check Selenium Grid node availability
- Verify external service health during failures

// Analyze test isolation
- Run failing tests individually vs. in suite
- Check for test order dependencies
- Verify test data cleanup</code></pre>
                        </li>
                        <li><strong>Resolution:</strong>
                            <ul>
                                <li>Fix root cause (not just add retries)</li>
                                <li>Add monitoring for early detection</li>
                                <li>Document findings for team</li>
                            </ul>
                        </li>
                    </ol>

                    <p><strong>Trade-offs:</strong> Quick retries get pipeline green but mask issues. Thorough investigation takes time but prevents recurring problems.</p>
                </div>
            </details>

            <details>
                <summary>S2: Playback works on Web but fails on Fire TV. How do you troubleshoot?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Test cross-platform debugging skills, understanding of device-specific constraints, and streaming technology knowledge.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Identify Symptoms:</strong>
                            <ul>
                                <li>Complete failure or specific issue (buffering, crash, quality)?</li>
                                <li>All content or specific content types?</li>
                                <li>All Fire TV models or specific generation?</li>
                            </ul>
                        </li>
                        <li><strong>Compare Environments:</strong>
                            <table>
                                <tr><th>Factor</th><th>Web</th><th>Fire TV</th></tr>
                                <tr><td>DRM</td><td>Widevine L3</td><td>Widevine L1/L3</td></tr>
                                <tr><td>Video Decoder</td><td>Software</td><td>Hardware</td></tr>
                                <tr><td>Memory</td><td>8GB+</td><td>1-2GB</td></tr>
                                <tr><td>Connection</td><td>Ethernet/WiFi</td><td>Usually WiFi only</td></tr>
                            </table>
                        </li>
                        <li><strong>Common Fire TV-Specific Issues:</strong>
                            <ul>
                                <li>Hardware codec limitations (e.g., no VP9 on older models)</li>
                                <li>DRM security level differences</li>
                                <li>WiFi interference from TV location</li>
                                <li>Memory pressure from background apps</li>
                            </ul>
                        </li>
                        <li><strong>Debug Steps:</strong>
<pre><code>// 1. Capture logs
adb logcat | grep -E "(ExoPlayer|MediaDrm|Video)"

// 2. Check manifest compatibility
- Verify HLS profile supported by device
- Check if specific quality level fails

// 3. Test with reduced quality
- Force 720p to rule out 4K codec issue
- Test without HDR

// 4. Compare network behavior
- Use Charles Proxy on same network
- Compare request/response headers</code></pre>
                        </li>
                        <li><strong>Resolution Path:</strong>
                            <ul>
                                <li>If codec issue: Add device-specific encoding profile</li>
                                <li>If DRM issue: Verify license server returns correct security level</li>
                                <li>If memory issue: Optimize app memory usage, add device to low-memory tier</li>
                            </ul>
                        </li>
                    </ol>
                </div>
            </details>

            <details>
                <summary>S3: API tests pass but UI tests fail for the same functionality. What's your approach?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Understand testing layers, ability to isolate issues between frontend/backend.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Understand the Gap:</strong>
                            <ul>
                                <li>API test verifies backend returns correct data</li>
                                <li>UI test verifies end-user experience</li>
                                <li>Gap must be in: UI rendering, data binding, or test implementation</li>
                            </ul>
                        </li>
                        <li><strong>Investigation:</strong>
<pre><code>// 1. Verify same environment
- Are API and UI tests hitting same backend?
- Any caching layer between them?

// 2. Inspect what UI receives
- Open browser DevTools Network tab
- Compare API response to what API test validated

// 3. Check UI rendering
- Is data received but not displayed?
- JavaScript error preventing render?
- CSS hiding elements?

// 4. Compare test assertions
- API test: response.body.title == "Movie"
- UI test: page.getText(titleElement) == "Movie"
- Are they checking same data transformation?</code></pre>
                        </li>
                        <li><strong>Common Causes:</strong>
                            <ul>
                                <li><strong>Timing:</strong> UI test doesn't wait for async data load</li>
                                <li><strong>Data transformation:</strong> Frontend formats data differently (e.g., date format)</li>
                                <li><strong>Caching:</strong> UI uses cached data, API hits fresh</li>
                                <li><strong>State:</strong> UI requires logged-in state API test doesn't</li>
                                <li><strong>A/B testing:</strong> UI shows different variant than expected</li>
                            </ul>
                        </li>
                        <li><strong>For Streaming Platform:</strong>
<pre><code>// Specific example: Content availability
API: GET /content/123 → 200 OK, available: true
UI: "Content not available" displayed

Possible causes:
- Geo-blocking applied at UI layer based on IP
- DRM license check fails in browser
- User subscription check in frontend
- Content scheduled for future release</code></pre>
                        </li>
                    </ol>
                </div>
            </details>

            <details>
                <summary>S4: Release is blocked 2 hours before deadline due to a critical bug. What do you do?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Evaluate crisis management, prioritization, and communication skills.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Immediate Assessment (15 mins):</strong>
                            <ul>
                                <li>Confirm bug is reproducible and truly critical</li>
                                <li>Identify exact user impact and affected scope</li>
                                <li>Determine if it's a regression or new bug</li>
                            </ul>
                        </li>
                        <li><strong>Communicate (5 mins):</strong>
                            <ul>
                                <li>Alert release manager, product owner, and development lead</li>
                                <li>Provide factual assessment without panic</li>
                                <li>"We found X issue affecting Y users. Here are our options..."</li>
                            </ul>
                        </li>
                        <li><strong>Evaluate Options:</strong>
                            <table>
                                <tr><th>Option</th><th>Pros</th><th>Cons</th></tr>
                                <tr><td>Fix and release</td><td>Full functionality</td><td>Rush fix is risky</td></tr>
                                <tr><td>Release without feature</td><td>Lower risk</td><td>Feature unavailable</td></tr>
                                <tr><td>Delay release</td><td>No risk</td><td>Business impact</td></tr>
                                <tr><td>Partial rollout</td><td>Limit exposure</td><td>Operational complexity</td></tr>
                            </table>
                        </li>
                        <li><strong>Support Resolution:</strong>
                            <ul>
                                <li>If fixing: Fast-track testing of the fix (focused, not full regression)</li>
                                <li>Prepare rollback plan if issues arise post-release</li>
                                <li>Monitor dashboards closely after release</li>
                            </ul>
                        </li>
                        <li><strong>Post-Mortem:</strong>
                            <ul>
                                <li>Why wasn't this caught earlier?</li>
                                <li>What test coverage gap exists?</li>
                                <li>Improve process to prevent recurrence</li>
                            </ul>
                        </li>
                    </ol>

                    <p><strong>Key Message:</strong> QA's role is to inform decision-makers with accurate information, not to unilaterally block releases. Provide data and recommendations, then support whatever decision is made.</p>
                </div>
            </details>

            <details>
                <summary>S5: You're asked to achieve 90% test coverage. How do you approach this?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Test understanding that coverage metrics aren't the goal, quality is.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Clarify Intent:</strong>
                            <ul>
                                <li>"What type of coverage? Line, branch, requirement, risk?"</li>
                                <li>High code coverage ≠ high quality</li>
                                <li>100% coverage with bad assertions is useless</li>
                            </ul>
                        </li>
                        <li><strong>Propose Risk-Based Approach:</strong>
<pre><code>// Prioritization matrix
High Risk + High Frequency → Must have 90%+ coverage
High Risk + Low Frequency → 80%+ coverage
Low Risk + High Frequency → 70%+ coverage
Low Risk + Low Frequency → 50%+ coverage

// For streaming platform
Critical: Playback, Authentication, Payment → 95%
Important: Search, Recommendations → 80%
Nice-to-have: Settings, Help → 60%</code></pre>
                        </li>
                        <li><strong>Meaningful Coverage Strategy:</strong>
                            <ul>
                                <li>Focus on behavior coverage, not just line coverage</li>
                                <li>Ensure assertions validate actual outcomes</li>
                                <li>Cover edge cases and error paths, not just happy paths</li>
                                <li>Include integration points and external dependencies</li>
                            </ul>
                        </li>
                        <li><strong>Implementation Plan:</strong>
<pre><code>// Phase 1: Identify gaps
- Run coverage analysis (JaCoCo, SonarQube)
- Map uncovered code to features
- Prioritize by risk

// Phase 2: Strategic coverage
- Unit tests for utility functions
- Integration tests for service boundaries
- E2E tests for critical user journeys

// Phase 3: Maintain quality
- Coverage gates in CI (fail if drops below threshold)
- Mutation testing to verify assertion quality
- Regular review of test effectiveness</code></pre>
                        </li>
                        <li><strong>Push Back Constructively:</strong>
                            <ul>
                                <li>"90% coverage is achievable, but I'd recommend we also track mutation score and defect escape rate to ensure the coverage is meaningful"</li>
                                <li>Propose metrics dashboard showing coverage + quality indicators together</li>
                            </ul>
                        </li>
                    </ol>

                    <div class="warning">Chasing coverage numbers leads to tests like <code>assertEquals(true, true)</code> that inflate metrics without adding value.</div>
                </div>
            </details>

            <details>
                <summary>S6: A test that was passing for months suddenly starts failing. No code changes. What do you investigate?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Assess understanding of environmental factors, dependency management, and systematic debugging.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Verify "No Code Changes" Claim:</strong>
                            <ul>
                                <li>Check all repositories: app code, test code, infrastructure</li>
                                <li>Review dependency updates (transitive dependencies)</li>
                                <li>Check for configuration changes</li>
                            </ul>
                        </li>
                        <li><strong>Environmental Factors:</strong>
<pre><code>// Common causes
1. Test data changed/expired
   - User accounts expired
   - Content licenses expired
   - Test content removed

2. External service changes
   - Third-party API contract changed
   - CDN configuration updated
   - SSL certificate rotated

3. Infrastructure changes
   - Browser/driver version auto-updated
   - Selenium Grid node configuration
   - Cloud provider updates

4. Time-sensitive issues
   - Date-based logic (promotions, schedules)
   - Timezone changes (DST)
   - Token expiration</code></pre>
                        </li>
                        <li><strong>Streaming-Specific Causes:</strong>
                            <ul>
                                <li>Content rights expired (geo-restrictions changed)</li>
                                <li>Manifest URL structure changed</li>
                                <li>DRM license server updated</li>
                                <li>Encoding profile changed for test content</li>
                            </ul>
                        </li>
                        <li><strong>Investigation Steps:</strong>
<pre><code>// 1. Compare last passing vs first failing
git log --since="2024-01-01" --until="2024-01-02"

// 2. Check external dependencies
curl -I https://cdn.streaming.com/test-content/manifest.m3u8

// 3. Run locally with verbose logging
mvn test -Dtest=FailingTest -X

// 4. Check timestamps and dates in test
// Is test checking for content available "today"?

// 5. Verify test data still exists
SELECT * FROM test_content WHERE id = 'test-123';</code></pre>
                        </li>
                        <li><strong>Prevention:</strong>
                            <ul>
                                <li>Version lock dependencies where possible</li>
                                <li>Monitor external dependency health</li>
                                <li>Use stable test data with long expiration</li>
                                <li>Avoid time-sensitive assertions</li>
                            </ul>
                        </li>
                    </ol>
                </div>
            </details>

            <details>
                <summary>S7: Product wants to release tomorrow but test automation is only 50% complete. What do you recommend?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Evaluate risk assessment, pragmatic decision-making, and communication skills.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Assess What 50% Means:</strong>
<pre><code>// Scenario A: 50% of planned tests, but all critical paths covered
✓ Login/Authentication
✓ Core playback flow
✓ Payment processing
✗ Edge cases
✗ Error handling
✗ Performance tests
→ Lower risk, might be acceptable

// Scenario B: Random 50% coverage
✗ Some critical paths untested
→ Higher risk, needs discussion</code></pre>
                        </li>
                        <li><strong>Provide Risk Assessment:</strong>
                            <table>
                                <tr><th>What's Tested</th><th>What's Not Tested</th><th>Risk Level</th></tr>
                                <tr><td>Happy path playback</td><td>Error recovery</td><td>Medium</td></tr>
                                <tr><td>Web platform</td><td>Mobile, TV</td><td>High</td></tr>
                                <tr><td>Basic DRM</td><td>License renewal</td><td>Medium</td></tr>
                            </table>
                        </li>
                        <li><strong>Propose Options:</strong>
                            <ol>
                                <li><strong>Delay release:</strong> Complete automation, full confidence</li>
                                <li><strong>Hybrid testing:</strong> Supplement with targeted manual testing for uncovered areas</li>
                                <li><strong>Staged rollout:</strong> Release to 10%, monitor, expand</li>
                                <li><strong>Feature flag:</strong> Release with new features disabled until tested</li>
                            </ol>
                        </li>
                        <li><strong>My Recommendation:</strong>
<pre><code>// For a streaming platform tomorrow release:
1. Identify top 10 highest-risk untested areas
2. Conduct focused 4-hour manual testing session
3. Implement staged rollout (10% → 50% → 100%)
4. Set up enhanced monitoring/alerting
5. Have rollback plan ready
6. Complete remaining automation post-release</code></pre>
                        </li>
                        <li><strong>Document the Decision:</strong>
                            <ul>
                                <li>Send email summarizing: risks, mitigations, who approved</li>
                                <li>This protects everyone and creates accountability</li>
                            </ul>
                        </li>
                    </ol>

                    <p><strong>Key Message:</strong> QA doesn't own the release decision, but we own the information needed to make it. Be a partner, not a gatekeeper.</p>
                </div>
            </details>

            <details>
                <summary>S8: How would you test a new "Download for Offline" feature on mobile?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Assess feature testing strategy, mobile-specific knowledge, and streaming domain understanding.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Understand Requirements:</strong>
                            <ul>
                                <li>Which content is downloadable? (DRM restrictions)</li>
                                <li>Download quality options?</li>
                                <li>Storage limits?</li>
                                <li>Offline viewing period?</li>
                                <li>Supported devices/OS versions?</li>
                            </ul>
                        </li>
                        <li><strong>Test Categories:</strong>

                            <h4>Functional Tests</h4>
<pre><code>// Download initiation
- Download single video
- Download entire series/season
- Download queue management
- Pause/resume download
- Cancel download
- Download over WiFi only setting

// Offline playback
- Play downloaded content offline
- Seek within downloaded content
- Subtitle display offline
- Audio track switching offline
- Playback controls work offline

// License/DRM
- Offline viewing period expires
- License renewal when back online
- Content removal after expiry</code></pre>

                            <h4>Edge Cases</h4>
<pre><code>// Storage
- Download when low storage
- Download size estimation accuracy
- Delete downloads to free space

// Network transitions
- WiFi → Cellular during download
- Network loss during download
- Resume after network recovery

// App lifecycle
- Download continues in background
- Download survives app kill
- Download persists after reboot</code></pre>

                            <h4>Performance Tests</h4>
<pre><code>// Download speed
- Multiple concurrent downloads
- Download priority handling

// Battery impact
- Battery usage during download
- Background download battery impact

// Storage impact
- Verify actual vs estimated size
- Cleanup of temporary files</code></pre>
                        </li>
                        <li><strong>Automation Approach:</strong>
<pre><code>// Appium test for download flow
@Test
public void testOfflinePlayback() {
    // Login
    app.login(testUser);

    // Download content
    app.navigateTo("movie-123");
    app.tapDownloadButton();
    app.selectQuality("720p");

    // Wait for download
    await().atMost(Duration.ofMinutes(10))
        .until(() -> app.getDownloadProgress("movie-123") == 100);

    // Go offline
    device.setNetworkConnection(NetworkConnection.AIRPLANE_MODE);

    // Play offline
    app.navigateToDownloads();
    app.play("movie-123");

    // Verify playback
    assertThat(app.isPlaying()).isTrue();
    sleep(30_000); // Play for 30 seconds
    assertThat(app.getCurrentTime()).isGreaterThan(25);
}</code></pre>
                        </li>
                        <li><strong>Device Matrix:</strong>
                            <ul>
                                <li>iOS: iPhone SE (low storage), iPhone 14 Pro</li>
                                <li>Android: Low-end (2GB RAM), flagship, Samsung, Pixel</li>
                                <li>Tablets: iPad, Android tablets</li>
                            </ul>
                        </li>
                    </ol>
                </div>
            </details>

            <details>
                <summary>S9: Users report video quality is poor despite having fast internet. How do you investigate?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Test understanding of ABR, debugging skills, and user-centric thinking.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Gather Information:</strong>
                            <ul>
                                <li>How many users? Specific regions? Devices?</li>
                                <li>What content? All or specific titles?</li>
                                <li>What ISPs are affected?</li>
                                <li>What's their definition of "poor quality"?</li>
                            </ul>
                        </li>
                        <li><strong>Potential Causes:</strong>
<pre><code>// Client-side
- ABR algorithm stuck on low quality
- Browser/app bug in quality selection
- Device capability detection wrong
- User setting limiting quality

// Server-side
- High-quality renditions not encoded
- Manifest serving wrong variants
- CDN serving stale content
- Throttling by content type/region

// Network
- ISP throttling video traffic
- DNS resolution to far CDN
- Packet loss despite high bandwidth
- WiFi interference</code></pre>
                        </li>
                        <li><strong>Investigation Steps:</strong>
<pre><code>// 1. Reproduce the issue
- Use same device/browser/ISP
- Use VPN to simulate region

// 2. Capture diagnostics
- Network waterfall (Chrome DevTools)
- Manifest responses
- Segment download speeds
- Player logs

// 3. Check server-side
- CDN logs for affected users
- Verify all quality renditions exist
- Check A/B test assignments

// 4. Analyze ABR behavior
- Is bandwidth estimation accurate?
- Is buffer staying healthy?
- Any errors in quality switching?</code></pre>
                        </li>
                        <li><strong>Common Streaming Issues:</strong>
                            <table>
                                <tr><th>Symptom</th><th>Likely Cause</th><th>Solution</th></tr>
                                <tr><td>Always 480p</td><td>Device detected as low-capability</td><td>Fix device detection</td></tr>
                                <tr><td>Quality never improves</td><td>ABR stuck, not receiving bandwidth signals</td><td>Debug ABR algorithm</td></tr>
                                <tr><td>1080p but blurry</td><td>Encoding quality issue</td><td>Re-encode content</td></tr>
                                <tr><td>Random quality drops</td><td>Network packet loss</td><td>CDN optimization</td></tr>
                            </table>
                        </li>
                        <li><strong>Testing for Prevention:</strong>
<pre><code>// Add quality monitoring
@Test
public void testQualityMeetsExpectation() {
    networkController.setThroughput(20_000_000); // 20 Mbps

    videoPlayer.loadContent("4k-movie");
    videoPlayer.play();

    // After stabilization, should be at highest quality
    await().atMost(Duration.ofSeconds(60))
        .until(() -> videoPlayer.getCurrentQualityHeight() >= 1080);

    // Should maintain for 2 minutes
    for (int i = 0; i < 24; i++) {
        sleep(5_000);
        assertThat(videoPlayer.getCurrentQualityHeight())
            .as("Quality should stay at 1080p+")
            .isGreaterThanOrEqualTo(1080);
    }
}</code></pre>
                        </li>
                    </ol>
                </div>
            </details>

            <details>
                <summary>S10: You need to test integration with a third-party recommendation engine. How do you approach this?</summary>
                <div class="answer">
                    <p><strong>Interviewer Intent:</strong> Assess integration testing strategy, mocking approaches, and handling of external dependencies.</p>

                    <h4>Structured Response:</h4>

                    <ol>
                        <li><strong>Understand the Integration:</strong>
                            <ul>
                                <li>API contract: What requests/responses?</li>
                                <li>Data flow: User data in → recommendations out</li>
                                <li>SLAs: Latency, availability requirements</li>
                                <li>Failure modes: What if service is down?</li>
                            </ul>
                        </li>
                        <li><strong>Testing Layers:</strong>
<pre><code>// Layer 1: Contract Testing
- Validate API adheres to agreed schema
- Use Pact or OpenAPI validation
- Run against sandbox environment

// Layer 2: Integration Testing (with mocks)
- Mock third-party responses for deterministic testing
- Test various response scenarios
- Fast, reliable, in CI pipeline

// Layer 3: End-to-End Testing
- Test against actual third-party sandbox
- Limited scope, run less frequently
- Validate real integration works</code></pre>
                        </li>
                        <li><strong>Mock Strategy:</strong>
<pre><code>// WireMock for recommendation engine
stubFor(post(urlPathEqualTo("/api/recommendations"))
    .withRequestBody(matchingJsonPath("$.userId"))
    .willReturn(aResponse()
        .withStatus(200)
        .withBodyFile("recommendations-response.json")));

// Test various scenarios
@Test
public void testRecommendationsDisplay() {
    // Mock returns 10 recommendations
    stubRecommendations(10, "action-movies");

    homePage.load(testUser);

    assertThat(homePage.getRecommendationCarousel().getItemCount())
        .isEqualTo(10);
    assertThat(homePage.getRecommendationCarousel().getTitle())
        .contains("Action");
}

@Test
public void testEmptyRecommendations() {
    // Mock returns empty
    stubRecommendations(0, null);

    homePage.load(testUser);

    // Should gracefully hide section
    assertThat(homePage.hasRecommendationCarousel()).isFalse();
}

@Test
public void testRecommendationServiceTimeout() {
    // Mock delays response
    stubFor(post(urlPathEqualTo("/api/recommendations"))
        .willReturn(aResponse()
            .withFixedDelay(10000))); // 10 second delay

    homePage.load(testUser);

    // Page should load with fallback content
    assertThat(homePage.isLoaded()).isTrue();
    assertThat(homePage.hasFallbackContent()).isTrue();
}</code></pre>
                        </li>
                        <li><strong>Data Validation:</strong>
<pre><code>@Test
public void testRecommendationRelevance() {
    // User with known watch history
    User user = createUserWithHistory("action", "sci-fi");

    // Get recommendations
    List&lt;Content&gt; recs = recommendationApi.getRecommendations(user);

    // Validate relevance
    long relevantCount = recs.stream()
        .filter(c -> c.getGenres().contains("action") ||
                    c.getGenres().contains("sci-fi"))
        .count();

    assertThat(relevantCount)
        .as("At least 70% should be relevant genres")
        .isGreaterThan(recs.size() * 0.7);
}</code></pre>
                        </li>
                        <li><strong>Monitoring & Alerting:</strong>
                            <ul>
                                <li>Track third-party API latency in production</li>
                                <li>Alert on error rate spikes</li>
                                <li>Have circuit breaker for degraded experience</li>
                            </ul>
                        </li>
                    </ol>
                </div>
            </details>
        </div>
    </section>

    <!-- SECTION 7: RAPID-FIRE REVIEW -->
    <section>
        <h2><span class="emoji">7️⃣</span> Final Rapid-Fire Review</h2>
        <div class="section-content">
            <p>Quick Q&A for last-minute revision:</p>

            <div class="rapid-fire">
                <div class="rapid-item">
                    <div class="rapid-q">1. What is the difference between HLS and DASH?</div>
                    <div class="rapid-a">HLS uses .m3u8 manifests and .ts segments (Apple), DASH uses .mpd (XML) manifests and is codec-agnostic (MPEG standard). Both support ABR.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">2. What does ABR stand for and why is it important?</div>
                    <div class="rapid-a">Adaptive Bitrate Streaming. It automatically adjusts video quality based on network conditions to minimize buffering while maximizing quality.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">3. Name three DRM systems and their primary platforms.</div>
                    <div class="rapid-a">Widevine (Chrome, Android), FairPlay (Safari, iOS, Apple TV), PlayReady (Edge, Windows, Xbox).</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">4. What is the difference between implicit and explicit waits?</div>
                    <div class="rapid-a">Implicit: global timeout for all findElement calls. Explicit: specific condition-based wait for individual elements. Never mix them.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">5. What is StaleElementReferenceException and how do you prevent it?</div>
                    <div class="rapid-a">Thrown when a previously found element is no longer attached to DOM. Prevent by re-finding elements when needed instead of storing references.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">6. What is ThreadLocal and why is it used in test automation?</div>
                    <div class="rapid-a">ThreadLocal provides thread-specific variable storage. Used to isolate WebDriver instances in parallel test execution.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">7. What is the difference between REST Assured's given(), when(), then()?</div>
                    <div class="rapid-a">given(): request specification (headers, body). when(): HTTP action (GET, POST). then(): response validation (status, body assertions).</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">8. What is Playwright's main advantage over Selenium?</div>
                    <div class="rapid-a">Built-in auto-waiting, native network interception, faster execution via Chrome DevTools Protocol, better debugging with trace viewer.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">9. What is a flaky test and how do you identify one?</div>
                    <div class="rapid-a">A test that passes and fails intermittently without code changes. Identify by tracking pass/fail rates over time and analyzing failure patterns.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">10. What is the Page Object Model's main benefit?</div>
                    <div class="rapid-a">Separation of test logic from page structure. UI changes only require updates in page objects, not in every test.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">11. What is Shadow DOM and how do you handle it in Selenium?</div>
                    <div class="rapid-a">Encapsulated DOM tree that regular locators can't access. Use element.getShadowRoot() (Selenium 4+) or JavaScript to penetrate.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">12. What is the difference between unit, integration, and E2E tests?</div>
                    <div class="rapid-a">Unit: isolated function/class. Integration: multiple components together. E2E: full user journey through entire system.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">13. What is WireMock used for?</div>
                    <div class="rapid-a">Service virtualization/mocking. Simulates HTTP services for testing without real dependencies.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">14. What is video buffering and what causes it?</div>
                    <div class="rapid-a">Buffering occurs when playback catches up to downloaded content. Caused by network speed slower than video bitrate or ABR not adapting fast enough.</div>
                </div>

                <div class="rapid-item">
                    <div class="rapid-q">15. What is the STAR method for behavioral questions?</div>
                    <div class="rapid-a">Situation (context), Task (your responsibility), Action (what you did), Result (outcome with metrics).</div>
                </div>
            </div>

            <h3>Key Differences Quick Reference</h3>
            <table>
                <thead>
                    <tr><th>Concept A</th><th>Concept B</th><th>Key Difference</th></tr>
                </thead>
                <tbody>
                    <tr><td>Selenium</td><td>Playwright</td><td>WebDriver protocol vs CDP, manual vs auto-waiting</td></tr>
                    <tr><td>TestNG</td><td>JUnit 5</td><td>@DataProvider vs @ParameterizedTest, XML config vs annotations</td></tr>
                    <tr><td>HashMap</td><td>LinkedHashMap</td><td>No order guarantee vs insertion order preserved</td></tr>
                    <tr><td>Checked Exception</td><td>Unchecked Exception</td><td>Compile-time handling required vs runtime only</td></tr>
                    <tr><td>REST Assured</td><td>Postman</td><td>Code-based testing vs GUI-based testing</td></tr>
                    <tr><td>Smoke Test</td><td>Regression Test</td><td>Critical path sanity vs comprehensive coverage</td></tr>
                    <tr><td>BDD</td><td>TDD</td><td>Behavior specification vs code-first design</td></tr>
                    <tr><td>Mock</td><td>Stub</td><td>Verify interactions vs provide canned responses</td></tr>
                </tbody>
            </table>

            <h3>Numbers to Remember</h3>
            <ul>
                <li><strong>HTTP 200:</strong> Success</li>
                <li><strong>HTTP 201:</strong> Created</li>
                <li><strong>HTTP 400:</strong> Bad Request</li>
                <li><strong>HTTP 401:</strong> Unauthorized</li>
                <li><strong>HTTP 403:</strong> Forbidden</li>
                <li><strong>HTTP 404:</strong> Not Found</li>
                <li><strong>HTTP 500:</strong> Internal Server Error</li>
                <li><strong>HTTP 503:</strong> Service Unavailable</li>
                <li><strong>Typical startup time SLA:</strong> &lt;3 seconds</li>
                <li><strong>Acceptable buffering ratio:</strong> &lt;1% of play time</li>
                <li><strong>Smoke test duration:</strong> &lt;10 minutes</li>
            </ul>
        </div>
    </section>

    <footer style="text-align: center; padding: 2rem; color: var(--text-light); border-top: 1px solid var(--border); margin-top: 2rem;">
        <p><strong>Good luck, Mike!</strong></p>
        <p>Remember: Be confident, provide specific examples, and demonstrate both technical depth and leadership qualities.</p>
        <p style="font-size: 0.9rem; margin-top: 1rem;">Study Guide prepared for VERSANT / Fandango Senior SDET Interview</p>
    </footer>

</body>
</html>
